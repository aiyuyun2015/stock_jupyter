{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- last week we learnt the rolling model\n",
    "- this week we would learn ensemble methods and intraday trading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEAD_PATH = './'\n",
    "DATA_PATH = HEAD_PATH + \"data/stocks/\"\n",
    "SAVE_PATH = HEAD_PATH + \"ckpt/\"    # most of signal timeseries is here\n",
    "TEMP_PATH = SAVE_PATH + \"tmp pkl/\"\n",
    "OUTPUT_PATH = HEAD_PATH + 'output'\n",
    "\n",
    "# signal result atr: signal evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'stock_stats' from '/mnt/d/A-Share/tmp/stock_jupyter/stock_stats.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stock_helper import *\n",
    "from stock_stats import *\n",
    "from product_info import *\n",
    "from imp import reload  \n",
    "import stock_helper\n",
    "import stock_stats\n",
    "reload(stock_helper)\n",
    "reload(stock_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    " \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import dask\n",
    "from dask import compute, delayed\n",
    "import sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORE_NUM = int(os.environ['NUMBER_OF_PROCESSORS'])\n",
    "CORE_NUM = 10\n",
    "## 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_list = [\"600519\", \"000858\", \"000568\", \"600809\", \"002304\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "period = 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## parallel generate the distribution of a signal\n",
    "def par_get_all_signal(signal_name, file_list, product, period, SAVE_PATH=\"e:/intern\"):\n",
    "    n_files = len(file_list)\n",
    "    all_signal = np.array([])\n",
    "    for file in file_list:\n",
    "        S = load(SAVE_PATH+\"/tmp pkl/\"+product+\"/\"+signal_name+\"/\"+file) ## signal\n",
    "#         good = load(SAVE_PATH+\"/good pkl/\"+product+\"/\"+file) ## good singal\n",
    "#         signal = S[good]\n",
    "        signal = S\n",
    "        moving_average(signal,period)\n",
    "        chosen = (np.arange(len(signal))+1) % period==0\n",
    "        all_signal = np.concatenate((all_signal, signal[chosen]), axis=0)\n",
    "    save(all_signal, SAVE_PATH+\"/all signal/\"+product+\".\"+signal_name+\".pkl\")\n",
    "    \n",
    "    \n",
    "def evaluate_signal(signal, all_dates, product, min_pnl, min_num, \n",
    "                    CORE_NUM, HEAD_PATH=\"d:/intern\", SIGNAL_PATH=\"d:/intern\", period=4096, split_str=\"2018\", \n",
    "                    atr_filter=0, save_path=\"signal result\",reverse=0):\n",
    "    signal_name = signal+\".\"+str(period) ## signal name, with period\n",
    "    all_signal = load(SIGNAL_PATH+\"/all signal/\"+product+\".\"+signal_name+\".pkl\") ## get the distribution of the signal\n",
    "    open_list = np.quantile(abs(all_signal), np.append(np.arange(0.991,0.999,0.001),np.arange(0.9991,0.9999,0.0001))) ## open threshold\n",
    "    thre_list = []\n",
    "    for cartesian in itertools.product(open_list, np.array([0.2, 0.4, 0.6, 0.8, 1.0])): ## close threshold\n",
    "        thre_list.append((cartesian[0], -cartesian[0] * cartesian[1]))\n",
    "    thre_list = np.array(thre_list)\n",
    "    thre_mat = pd.DataFrame(data=OrderedDict([(\"open\", thre_list[:, 0]), (\"close\", thre_list[:, 1])])) ## threshold matrix\n",
    "    \n",
    "    if reverse>=0: ## trending signal\n",
    "        print(\"reverse=1\")\n",
    "        trend_signal_stat = get_signal_stat(signal_name, thre_mat, product, all_dates, CORE_NUM, split_str=split_str, reverse=1, \n",
    "                                    atr_filter=atr_filter, HEAD_PATH=HEAD_PATH, SAVE_PATH=SIGNAL_PATH)\n",
    "    if reverse<=0: ## reversal signal\n",
    "        print(\"reverse=-1\")\n",
    "        reverse_signal_stat = get_signal_stat(signal_name, thre_mat, product, all_dates, CORE_NUM, split_str=split_str, reverse=-1,\n",
    "                                            atr_filter=atr_filter, HEAD_PATH=HEAD_PATH, SAVE_PATH=SIGNAL_PATH)\n",
    "    if reverse==0: ## both trending and reversal\n",
    "        stat_result = OrderedDict([(\"trend.signal.stat\", trend_signal_stat), (\"reverse.signal.stat\", reverse_signal_stat)])    \n",
    "        save(stat_result, HEAD_PATH+\"/\"+save_path+\"/\"+product+\".\"+signal_name+\".pkl\")\n",
    "    elif reverse==1: ## just trend\n",
    "        save(trend_signal_stat, HEAD_PATH+\"/\"+save_path+\"/\"+product+\".\"+signal_name+\".trend.pkl\")\n",
    "    elif reverse==-1: ## just reversal\n",
    "        save(reverse_signal_stat, HEAD_PATH+\"/\"+save_path+\"/\"+product+\".\"+signal_name+\".reverse.pkl\")\n",
    "        \n",
    "\n",
    "# calc pnl given a signal\n",
    "def get_signal_pnl(file, product, signal_name, thre_mat, reverse=1,  buy_tranct=1.5e-4, sell_tranct=11.5e-4,\n",
    "                   max_spread=0.011,\n",
    "                   HEAD_PATH=HEAD_PATH, SAVE_PATH=SAVE_PATH, atr_filter=0):\n",
    "    ## load data\n",
    "    data = load(DATA_PATH+product+\"/\"+file)\n",
    "    S = load(SAVE_PATH+\"/tmp pkl/\"+product+\"/\"+signal_name+\"/\"+file)\n",
    "    pred = S*reverse\n",
    "    atr = load(SAVE_PATH+\"/tmp pkl/\"+product+\"/\"+\"atr.4096\"+\"/\"+file)\n",
    "    #n_bar = len(data)\n",
    "    \n",
    "    ## load signal\n",
    "    \n",
    "    ## we don't know the signal is positive correlated or negative correlated  \n",
    "    #n_thre = len(thre_mat)\n",
    "    date = np.array([x[0:10] for x in data[\"date.time\"]])\n",
    "    next_date = np.append(date[1:],'1')\n",
    "    end_day = date!=next_date\n",
    "    count = 0;\n",
    "    n_day = sum(end_day)\n",
    "    n_thre = np.shape(thre_mat)[0]\n",
    "    all_pnl = np.zeros((n_day, n_thre))\n",
    "    result = pd.DataFrame(data=OrderedDict([(\"open\", thre_mat[\"open\"].values), (\"close\", thre_mat[\"close\"].values),\n",
    "                               (\"num\", 0), (\"avg.ret\", 0), (\"ret\", 0)]), \n",
    "                          index=thre_mat.index)\n",
    "\n",
    "    cur_spread = data[\"ask1\"]-data[\"bid1\"]\n",
    "    for thre in thre_mat.iterrows():\n",
    "        count = count+1\n",
    "        buy = pred>thre[1][\"open\"]\n",
    "        sell = pred<-thre[1][\"open\"]\n",
    "        signal = pd.Series(data=0, index=data.index)\n",
    "        position = signal.copy()\n",
    "        signal[buy] = 1\n",
    "        signal[sell] = -1\n",
    "        signal[atr<atr_filter]=0\n",
    "        scratch = -thre[1][\"close\"]\n",
    "        position_pos = pd.Series(data=np.nan, index=data.index)\n",
    "        position_pos.iloc[0] = 0\n",
    "        position_pos[(signal==1) & (data[\"next.ask\"]>0) & (data[\"next.bid\"]>0) & (cur_spread<max_spread)] = 1\n",
    "        position_pos[(pred< -scratch) & (data[\"next.bid\"]>0) & (cur_spread<max_spread)] = 0\n",
    "        position_pos.ffill(inplace=True)\n",
    "        pre_pos = position_pos.shift(1)\n",
    "        notional_position_pos = pd.Series(data=0, index=data.index)\n",
    "        notional_position_pos[position_pos==1] = 1\n",
    "        notional_position_pos[(position_pos==1) & (pre_pos==1)] = np.nan\n",
    "        notional_position_pos[(notional_position_pos==1)] = 1/data[\"next.ask\"][(notional_position_pos==1)]\n",
    "        notional_position_pos.ffill(inplace=True)\n",
    "        position_neg = pd.Series(data=np.nan, index=data.index)\n",
    "        position_neg.iloc[0] = 0\n",
    "        position_neg[(signal==-1) & (data[\"next.ask\"]>0) & (data[\"next.bid\"]>0) & (cur_spread<max_spread)] = -1\n",
    "        position_neg[(pred> scratch) & (data[\"next.ask\"]>0) & (cur_spread<max_spread)] = 0\n",
    "        position_neg.ffill(inplace=True)\n",
    "        pre_neg = position_neg.shift(1)\n",
    "        notional_position_neg = pd.Series(data=0, index=data.index)\n",
    "        notional_position_neg[position_neg==-1] = -1\n",
    "        notional_position_neg[(position_neg==-1) & (pre_neg==-1)] = np.nan\n",
    "        notional_position_neg[(notional_position_neg==-1)] = -1/data[\"next.bid\"][(notional_position_neg==-1)]\n",
    "        notional_position_neg.ffill(inplace=True)\n",
    "        position = position_pos + position_neg\n",
    "        notional_position = notional_position_pos+notional_position_neg\n",
    "        #position[n_bar-1] = 0\n",
    "        position.iloc[0] = 0\n",
    "        position.iloc[-2:] = 0\n",
    "        notional_position.iloc[0] = 0\n",
    "        notional_position.iloc[-2:] = 0\n",
    "        #change_pos = position - position.shift(1)\n",
    "        #notional_change_pos = notional_position-notional_position.shift(1)\n",
    "        change_pos = notional_position-notional_position.shift(1)\n",
    "        change_pos.iloc[0] = 0\n",
    "        #notional_change_pos.iloc[0] = 0\n",
    "        change_base = pd.Series(data=0, index=data.index)\n",
    "        change_buy = change_pos>0\n",
    "        change_sell = change_pos<0\n",
    "        change_base[change_buy] = data[\"next.ask\"][change_buy]*(1+buy_tranct)*data[\"adjust\"]\n",
    "        change_base[change_sell] = data[\"next.bid\"][change_sell]*(1-sell_tranct)*data[\"adjust\"]\n",
    "        raw_pnl = -(change_base*change_pos).cumsum()+notional_position*data[\"wpr\"]\n",
    "        final_pnl = -sum(change_base*change_pos) ## total pnl, there is a negative sign, because selling get money and buying pay money\n",
    "        turnover = sum(change_base*abs(change_pos))\n",
    "        num = sum((position!=0) & (change_pos!=0)) ## number of trades\n",
    "        hld_period = sum(position!=0)   ## holding period\n",
    "        daily_pnl = raw_pnl[end_day].reset_index(drop=True)\n",
    "        pnl = np.append(daily_pnl[0], np.diff(daily_pnl))\n",
    "        all_pnl[:,thre[0]] = pnl\n",
    "        if (num==0):\n",
    "            result.loc[thre[0], (\"num\",\"avg.ret\",\"ret\")] = (0,0,0)\n",
    "        else:\n",
    "            result.loc[thre[0],(\"num\", \"avg.ret\", \"ret\", )] = (num, final_pnl/num, final_pnl)\n",
    "    return OrderedDict([(\"all.pnl\", all_pnl), (\"result\", result), (\"date\", date[end_day])])\n",
    "\n",
    "    \n",
    "from collections import OrderedDict\n",
    "def get_signal_stat(signal_name, thre_mat, product, all_dates, CORE_NUM, split_str=\"2018\", reverse=1, \n",
    "                     atr_filter=0, HEAD_PATH=\"d:/intern\", SAVE_PATH=\"d:/intern\"):\n",
    "    train_sample = all_dates<split_str ## training samples\n",
    "    test_sample = all_dates>split_str ## testing samples\n",
    "    with dask.config.set(scheduler='processes', num_workers=CORE_NUM):\n",
    "        f_par = functools.partial(get_signal_pnl, product=product, signal_name=signal_name, thre_mat=thre_mat,\n",
    "                                 reverse=1,\n",
    "                                HEAD_PATH=HEAD_PATH, SAVE_PATH=SAVE_PATH,atr_filter=atr_filter)\n",
    "        train_result = compute([delayed(f_par)(file) for file in all_dates[train_sample]])[0] ## get training result\n",
    "    train_stat = get_hft_summary(train_result, thre_mat) ## get training result statistics\n",
    "    with dask.config.set(scheduler='processes', num_workers=CORE_NUM):\n",
    "        f_par = functools.partial(get_signal_pnl, product=product, signal_name=signal_name, thre_mat=thre_mat,\n",
    "                                 reverse=1,\n",
    "                                HEAD_PATH=HEAD_PATH, SAVE_PATH=SAVE_PATH,atr_filter=atr_filter)\n",
    "        test_result = compute([delayed(f_par)(file) for file in all_dates[test_sample]])[0] ## get testing result\n",
    "    test_stat = get_hft_summary(test_result, thre_mat) ## get testing result statistics\n",
    "    return OrderedDict([(\"train.stat\", train_stat), (\"test.stat\", test_stat)])\n",
    "\n",
    "    \n",
    "def evaluate_signal(signal, all_dates, product, min_pnl, min_num, \n",
    "                    CORE_NUM, HEAD_PATH=\"d:/intern\", SIGNAL_PATH=\"d:/intern\", period=4096, split_str=\"2018\", \n",
    "                    atr_filter=0, save_path=\"signal result\",reverse=0):\n",
    "    signal_name = signal+\".\"+str(period) ## signal name, with period\n",
    "    all_signal = load(SIGNAL_PATH+\"/all signal/\"+product+\".\"+signal_name+\".pkl\") ## get the distribution of the signal\n",
    "    open_list = np.quantile(abs(all_signal), np.append(np.arange(0.991,0.999,0.001),np.arange(0.9991,0.9999,0.0001))) ## open threshold\n",
    "    thre_list = []\n",
    "    for cartesian in itertools.product(open_list, np.array([0.2, 0.4, 0.6, 0.8, 1.0])): ## close threshold\n",
    "        thre_list.append((cartesian[0], -cartesian[0] * cartesian[1]))\n",
    "    thre_list = np.array(thre_list)\n",
    "    thre_mat = pd.DataFrame(data=OrderedDict([(\"open\", thre_list[:, 0]), (\"close\", thre_list[:, 1])])) ## threshold matrix\n",
    "    \n",
    "    if reverse>=0: ## trending signal\n",
    "        print(\"reverse=1\")\n",
    "        trend_signal_stat = get_signal_stat(signal_name, thre_mat, product, all_dates, CORE_NUM, split_str=split_str, reverse=1, \n",
    "                                    atr_filter=atr_filter, HEAD_PATH=HEAD_PATH, SAVE_PATH=SIGNAL_PATH)\n",
    "    if reverse<=0: ## reversal signal\n",
    "        print(\"reverse=-1\")\n",
    "        reverse_signal_stat = get_signal_stat(signal_name, thre_mat, product, all_dates, CORE_NUM, split_str=split_str, reverse=-1,\n",
    "                                            atr_filter=atr_filter, HEAD_PATH=HEAD_PATH, SAVE_PATH=SIGNAL_PATH)\n",
    "    if reverse==0: ## both trending and reversal\n",
    "        stat_result = OrderedDict([(\"trend.signal.stat\", trend_signal_stat), (\"reverse.signal.stat\", reverse_signal_stat)])    \n",
    "        save(stat_result, HEAD_PATH+\"/\"+save_path+\"/\"+product+\".\"+signal_name+\".pkl\")\n",
    "    elif reverse==1: ## just trend\n",
    "        save(trend_signal_stat, HEAD_PATH+\"/\"+save_path+\"/\"+product+\".\"+signal_name+\".trend.pkl\")\n",
    "    elif reverse==-1: ## just reversal\n",
    "        save(reverse_signal_stat, HEAD_PATH+\"/\"+save_path+\"/\"+product+\".\"+signal_name+\".reverse.pkl\")\n",
    "        \n",
    "# summarize pnl over thresholds\n",
    "def get_hft_summary(result, thre_mat):\n",
    "    n_thre = np.shape(thre_mat)[0]\n",
    "    all_pnl = np.zeros((0,n_thre))\n",
    "    all_dates = np.array([])\n",
    "    for i in range(len(result)):\n",
    "        all_pnl =  np.concatenate((all_pnl,result[i][\"all.pnl\"]),axis=0)\n",
    "        all_dates = np.append(all_dates, result[i][\"date\"])\n",
    "    stat = result[0][\"result\"].iloc[:,2:]\n",
    "    for i in range(1,len(result)):\n",
    "        stat = stat+result[i][\"result\"].iloc[:,2:]\n",
    "    stat[\"avg.ret\"]=stat[\"ret\"]/stat[\"num\"]\n",
    "    \n",
    "    total_ret = all_pnl.sum(0)\n",
    "    total_sharpe = np.zeros(n_thre)\n",
    "    total_drawdown = np.zeros(n_thre)\n",
    "    total_max_drawdown = np.zeros(n_thre)\n",
    "    for i in range(n_thre):\n",
    "        total_sharpe[i] = sharpe(all_pnl[:,i])\n",
    "        total_drawdown[i] = drawdown(all_pnl[:,i])\n",
    "        total_max_drawdown[i] = max_drawdown(all_pnl[:,i])\n",
    "    final_result = pd.DataFrame(data=OrderedDict([(\"open\", thre_mat[\"open\"]), (\"close\", thre_mat[\"close\"]), (\"num\", stat[\"num\"]),\n",
    "                                                  (\"avg.ret\", stat[\"avg.ret\"]), (\"total.ret\",total_ret), (\"sharpe\", total_sharpe),\n",
    "                                                  (\"drawdown\", total_drawdown), (\"max.drawdown\", total_max_drawdown),\n",
    "                                                 (\"mar\", total_ret/total_max_drawdown)]), \n",
    "                                index=thre_mat.index)\n",
    "    return OrderedDict([(\"final.result\", final_result), (\"daily.pnl\", all_pnl), (\"date\", all_dates)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## coutn the number of signals of every day\n",
    "# def count_daily_num(x, period):\n",
    "#     data = load(x)\n",
    "#     return np.floor(np.shape(data)[0]/period)\n",
    "\n",
    "## get multiple samples of the singal matrix\n",
    "def get_multiple_sample_signal(good_night_files, sample, product, signal_name, period, daily_num, mod=np.array([0])):\n",
    "    n_samples = int(sum(daily_num[sample])) ## number of samples\n",
    "    n_mod = len(mod) ## number of different copies of the same signal\n",
    "    if n_mod==1: ## if there is one copy then the same as before\n",
    "        all_signal = np.zeros(n_samples)\n",
    "    else:\n",
    "        all_signal = np.zeros((n_mod, n_samples)) ## if there are multiple copies then use a matrix for one signal instead\n",
    "    cur = 0\n",
    "    for file in good_night_files[sample]: ## iterate for files\n",
    "        S = load(SAVE_PATH+\"/tmp pkl/\"+product+\"/\"+signal_name+\"/\"+file)\n",
    "        for i in range(n_mod):\n",
    "            signal = S[(np.arange(len(S))+1) % period == mod[i]]\n",
    "            ## the ret.cor has some bad records\n",
    "            signal[np.isnan(signal)] = 0\n",
    "            signal[np.isinf(signal)] = 0\n",
    "            if mod[i]==0: \n",
    "                cur_end = np.min((cur+len(signal),n_samples)) ## to avoid exceed the number of samoes, mod[i]==0 has most signals, \n",
    "                ## so if it doesn't exceed, others would not exceed\n",
    "            if n_mod==1:\n",
    "                all_signal[cur:cur_end] = signal[0:(cur_end-cur)]\n",
    "            else:\n",
    "                all_signal[i, cur:cur_end] = signal[0:(cur_end-cur)]\n",
    "        cur = cur_end\n",
    "    return all_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dates = os.listdir(DATA_PATH + product_list[0])\n",
    "all_dates.sort()\n",
    "all_dates = np.array(all_dates)\n",
    "n_days = len(all_dates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE_PATH = \"e:/intern\"\n",
    "all_dates_x = os.listdir(DATA_PATH + product_list[0])\n",
    "all_dates_y = os.listdir(DATA_PATH + product_list[1])\n",
    "all_dates = np.array(list(set(all_dates_x) & set(all_dates_y)))\n",
    "all_dates.sort()\n",
    "train_sample = all_dates<\"2017\"\n",
    "test_sample = all_dates>\"2017\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(SAVE_PATH+\"/all signal\", exist_ok=True)\n",
    "dire_signal_list = [\"nr\", \"dbook\", \"range.pos\", \"price.osci\", \"ma.dif.10\", \"kdj.k\", \"kdj.j\"]\n",
    "range_signal_list = [\"\", \"range\", \"std\", \"trend.index\"]\n",
    "all_signal_list = np.array([])\n",
    "for range_signal in range_signal_list:\n",
    "    for dire_signal in dire_signal_list:\n",
    "        if len(range_signal)==0:\n",
    "            signal_name = dire_signal\n",
    "        else:\n",
    "            signal_name = dire_signal+\".\"+range_signal\n",
    "        all_signal_list = np.append(all_signal_list,signal_name)\n",
    "signal_list = [signal+\".4096\" for signal in all_signal_list]\n",
    "np.append(signal_list, [\"ret.4096\", \"ret.4096.001\", \"ret.4096.002\"])\n",
    "n_signal = len(signal_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import lasso_path, LassoCV,RidgeCV\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#y_signal = \"ret.\"+str(period)+\".004\"\n",
    "y_signal = \"ret.\"+str(period)+\".002\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0.,  409.,  818., 1227., 1636., 2045., 2454., 2863., 3272.,\n",
       "       3681.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod = np.arange(0, np.floor(period/10)*10, np.floor(period/10))\n",
    "n_mod = len(mod)\n",
    "mod\n",
    "## array([   0.,  409.,  818., 1227., 1636., 2045., 2454., 2863., 3272.,\n",
    "##        3681.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE_PATH = \"e:/intern\"\n",
    "os.makedirs(SAVE_PATH+\"/train test array\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1st Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All sets of signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600519\n",
      "000858\n",
      "000568\n",
      "600809\n",
      "002304\n",
      "CPU times: user 1min 17s, sys: 8.13 s, total: 1min 25s\n",
      "Wall time: 4min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for product in product_list:\n",
    "    print(product)\n",
    "    daily_num = load(SAVE_PATH+\"/daily num/\"+product+\".pkl\")\n",
    "    n_train = sum(daily_num[train_sample])\n",
    "    n_test = sum(daily_num[test_sample])\n",
    "    train_array = np.zeros((n_mod, int(n_train), n_signal+1))\n",
    "    for i in range(n_signal):\n",
    "        train_array[:,:,i] = get_multiple_sample_signal(all_dates, train_sample, product, signal_list[i], period, daily_num, mod)\n",
    "    train_array[:,:,n_signal] = get_multiple_sample_signal(all_dates, train_sample, product, \"ret.\"+str(period), period, daily_num, mod)\n",
    "    save(train_array, SAVE_PATH+\"/train test array/\"+product+\".train.array.pkl\")\n",
    "    \n",
    "# 600519\n",
    "# 000858\n",
    "# 000568\n",
    "# 600809\n",
    "# 002304\n",
    "# Wall time: 2min 53s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_array = dict([])\n",
    "for product in product_list:\n",
    "    all_train_array[product] = load(SAVE_PATH+\"/train test array/\"+product+\".train.array.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coefficients of all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the result of 10 models\n",
    "## save them in to a model_coef dictionary\n",
    "nfold = 10\n",
    "model_coef = dict([])\n",
    "for product in product_list:\n",
    "    model_coef[product] = np.zeros(n_signal)\n",
    "for i_mod in range(n_mod):\n",
    "    train_std_mat = dict([])\n",
    "    x_train = np.zeros((0, n_signal))\n",
    "    y_train = np.array([])\n",
    "    for product in product_list:\n",
    "        cur_mat = all_train_array[product][i_mod,:,:]\n",
    "        train_std_mat[product] = np.std(cur_mat, axis=0)\n",
    "        x_train = np.vstack((x_train, cur_mat[:,:n_signal]/train_std_mat[product][:n_signal]))\n",
    "        y_train = np.concatenate((y_train, cur_mat[:,n_signal]/train_std_mat[product][n_signal]))\n",
    "    scaler =  StandardScaler(copy=True, with_mean=False, with_std=True)\n",
    "    scaler.fit(x_train)\n",
    "    x_std = np.sqrt(scaler.var_)\n",
    "    x_train_normal = scaler.transform(x_train)\n",
    "    model = LassoCV(n_alphas=100, fit_intercept=False, cv=10, max_iter=10000).fit(x_train_normal, y_train)\n",
    "    fit_coef = model.coef_/x_std\n",
    "    for product in product_list:\n",
    "        coef = fit_coef*train_std_mat[product][n_signal]/train_std_mat[product][:n_signal]\n",
    "        model_coef[product] = model_coef[product]+coef/n_mod\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(model_coef, SAVE_PATH+\"/wine.ensemble.model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_coef = load(SAVE_PATH+\"/wine.ensemble.model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'600519': array([ 6.56638999e-02,  8.67937178e-03,  1.12820595e-02, -3.45893863e-04,\n",
       "        -1.69496053e-01,  8.69035647e-04, -6.94687572e-04, -7.55077389e-03,\n",
       "        -2.68724390e-03, -1.85624470e-03, -3.88807539e-04,  9.17066951e-03,\n",
       "         2.31254444e-04, -2.70780985e-06, -1.40674802e-04, -1.03465109e-03,\n",
       "        -4.04187711e-04,  5.72359513e-04,  1.52929013e-02,  8.89410834e-05,\n",
       "         0.00000000e+00,  9.71755727e-02,  9.47852799e-03,  1.34725785e-03,\n",
       "         3.03069677e-03, -1.38021409e-01, -5.03610604e-05, -6.69183987e-04]),\n",
       " '000858': array([ 5.79486801e-02,  6.10839453e-03,  1.10598584e-02, -3.22407674e-04,\n",
       "        -1.69352624e-01,  8.31988703e-04, -6.82160049e-04, -6.52527383e-02,\n",
       "        -2.26125063e-02, -1.75662348e-02, -3.43397293e-03,  7.12125531e-02,\n",
       "         2.14439807e-03, -6.11855953e-05, -1.93167988e-03, -8.52036979e-03,\n",
       "        -3.75934734e-03,  4.94148310e-03,  1.37450318e-01,  8.45934873e-04,\n",
       "         0.00000000e+00,  8.62163041e-02,  6.29179737e-03,  1.28061939e-03,\n",
       "         2.96175100e-03, -1.42302079e-01, -5.25622524e-05, -6.52758840e-04]),\n",
       " '000568': array([ 8.32584057e-02,  9.84656518e-03,  1.46337913e-02, -4.23444174e-04,\n",
       "        -1.71655002e-01,  1.08816317e-03, -9.15574785e-04, -6.47137947e-02,\n",
       "        -2.65663540e-02, -1.63574481e-02, -3.30726612e-03,  6.26513661e-02,\n",
       "         1.99876226e-03, -4.23711339e-05, -1.46926791e-03, -9.92514438e-03,\n",
       "        -3.69774491e-03,  4.84810238e-03,  1.00029357e-01,  8.03378756e-04,\n",
       "         0.00000000e+00,  1.23994380e-01,  1.01834584e-02,  1.72775025e-03,\n",
       "         3.89680821e-03, -1.39928549e-01, -7.02822910e-05, -8.66623287e-04]),\n",
       " '600809': array([ 1.07634511e-01,  1.49424196e-02,  1.94785845e-02, -5.76943838e-04,\n",
       "        -1.63945984e-01,  1.49789162e-03, -1.23145725e-03, -3.61749942e-02,\n",
       "        -1.47730775e-02, -9.35304243e-03, -1.87144176e-03,  2.64820577e-02,\n",
       "         1.16682101e-03,  3.23497973e-06, -7.42753736e-04, -5.31543897e-03,\n",
       "        -1.95948488e-03,  2.69090959e-03,  4.66702819e-02,  4.47707009e-04,\n",
       "         0.00000000e+00,  1.61912945e-01,  1.49810619e-02,  2.26479427e-03,\n",
       "         5.14113786e-03, -1.31605803e-01, -8.56769785e-05, -1.14695631e-03]),\n",
       " '002304': array([ 1.03240934e-01,  1.65143820e-02,  1.82405970e-02, -4.99656748e-04,\n",
       "        -1.65016813e-01,  1.42759195e-03, -1.15023710e-03, -4.06148223e-03,\n",
       "        -5.65340222e-03, -2.37631008e-03, -5.01234678e-04,  4.52183688e-03,\n",
       "         3.15272028e-04,  4.74789451e-06,  2.48310890e-04, -1.47088241e-03,\n",
       "        -4.42081519e-04,  8.68033420e-04,  3.87059949e-03,  1.00663737e-04,\n",
       "         0.00000000e+00,  1.51970946e-01,  1.79941332e-02,  2.11798803e-03,\n",
       "         4.70395460e-03, -1.39872776e-01, -8.25842740e-05, -1.09728937e-03])}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_coef\n",
    "\n",
    "# {'600519': array([ 6.56638999e-02,  8.67937178e-03,  1.12820595e-02, -3.45893863e-04,\n",
    "#         -1.69496053e-01,  8.69035647e-04, -6.94687572e-04, -7.55077389e-03,\n",
    "#         -2.68724390e-03, -1.85624470e-03, -3.88807539e-04,  9.17066951e-03,\n",
    "#          2.31254444e-04, -2.70780985e-06, -1.40674802e-04, -1.03465109e-03,\n",
    "#         -4.04187711e-04,  5.72359513e-04,  1.52929013e-02,  8.89410834e-05,\n",
    "#          0.00000000e+00,  9.71755727e-02,  9.47852799e-03,  1.34725785e-03,\n",
    "#          3.03069677e-03, -1.38021409e-01, -5.03610604e-05, -6.69183987e-04]),\n",
    "#  '000858': array([ 5.79486801e-02,  6.10839453e-03,  1.10598584e-02, -3.22407674e-04,\n",
    "#         -1.69352624e-01,  8.31988703e-04, -6.82160049e-04, -6.52527383e-02,\n",
    "#         -2.26125063e-02, -1.75662348e-02, -3.43397293e-03,  7.12125531e-02,\n",
    "#          2.14439807e-03, -6.11855953e-05, -1.93167988e-03, -8.52036979e-03,\n",
    "#         -3.75934734e-03,  4.94148310e-03,  1.37450318e-01,  8.45934873e-04,\n",
    "#          0.00000000e+00,  8.62163041e-02,  6.29179737e-03,  1.28061939e-03,\n",
    "#          2.96175100e-03, -1.42302079e-01, -5.25622524e-05, -6.52758840e-04]),\n",
    "#  '000568': array([ 8.32584057e-02,  9.84656518e-03,  1.46337913e-02, -4.23444174e-04,\n",
    "#         -1.71655002e-01,  1.08816317e-03, -9.15574785e-04, -6.47137947e-02,\n",
    "#         -2.65663540e-02, -1.63574481e-02, -3.30726612e-03,  6.26513661e-02,\n",
    "#          1.99876226e-03, -4.23711339e-05, -1.46926791e-03, -9.92514438e-03,\n",
    "#         -3.69774491e-03,  4.84810238e-03,  1.00029357e-01,  8.03378756e-04,\n",
    "#          0.00000000e+00,  1.23994380e-01,  1.01834584e-02,  1.72775025e-03,\n",
    "#          3.89680821e-03, -1.39928549e-01, -7.02822910e-05, -8.66623287e-04]),\n",
    "#  '600809': array([ 1.07634511e-01,  1.49424196e-02,  1.94785845e-02, -5.76943838e-04,\n",
    "#         -1.63945984e-01,  1.49789162e-03, -1.23145725e-03, -3.61749942e-02,\n",
    "#         -1.47730775e-02, -9.35304243e-03, -1.87144176e-03,  2.64820577e-02,\n",
    "#          1.16682101e-03,  3.23497973e-06, -7.42753736e-04, -5.31543897e-03,\n",
    "#         -1.95948488e-03,  2.69090959e-03,  4.66702819e-02,  4.47707009e-04,\n",
    "#          0.00000000e+00,  1.61912945e-01,  1.49810619e-02,  2.26479427e-03,\n",
    "#          5.14113786e-03, -1.31605803e-01, -8.56769785e-05, -1.14695631e-03]),\n",
    "#  '002304': array([ 1.03240934e-01,  1.65143820e-02,  1.82405970e-02, -4.99656748e-04,\n",
    "#         -1.65016813e-01,  1.42759195e-03, -1.15023710e-03, -4.06148223e-03,\n",
    "#         -5.65340222e-03, -2.37631008e-03, -5.01234678e-04,  4.52183688e-03,\n",
    "#          3.15272028e-04,  4.74789451e-06,  2.48310890e-04, -1.47088241e-03,\n",
    "#         -4.42081519e-04,  8.68033420e-04,  3.87059949e-03,  1.00663737e-04,\n",
    "#          0.00000000e+00,  1.51970946e-01,  1.79941332e-02,  2.11798803e-03,\n",
    "#          4.70395460e-03, -1.39872776e-01, -8.25842740e-05, -1.09728937e-03])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat = \"wine.ensemble.lasso.\"+str(period)\n",
    "for product in product_list:\n",
    "    os.makedirs(SAVE_PATH+\"/tmp pkl/\"+product+\"/\"+strat, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600519\n",
      "000858\n",
      "000568\n",
      "600809\n",
      "002304\n",
      "CPU times: user 430 ms, sys: 284 ms, total: 715 ms\n",
      "Wall time: 2min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for product in product_list:\n",
    "    print(product)\n",
    "    coef = model_coef[product]\n",
    "    parLapply(CORE_NUM, all_dates, par_get_daily_pred, \n",
    "              product=product, coef=coef, strat=strat, HEAD_PATH=SAVE_PATH, SAVE_PATH=SAVE_PATH)\n",
    "\n",
    "# 600519\n",
    "# 000858\n",
    "# 000568\n",
    "# 600809\n",
    "# 002304\n",
    "# Wall time: 6min 11s\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.91 s, sys: 562 ms, total: 4.47 s\n",
      "Wall time: 18.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for product in product_list:\n",
    "    par_get_all_signal(strat, all_dates, product, 4096, SAVE_PATH=SAVE_PATH)\n",
    "\n",
    "## Wall time: 8.99 s\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reverse=1\n",
      "reverse=1\n",
      "reverse=1\n",
      "reverse=1\n",
      "reverse=1\n",
      "CPU times: user 536 ms, sys: 459 ms, total: 995 ms\n",
      "Wall time: 9min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for product in product_list:\n",
    "    evaluate_signal(\"wine.ensemble.lasso\", all_dates, product, 0.001, 20, CORE_NUM, SAVE_PATH, SAVE_PATH,\n",
    "                        period=4096, split_str=\"2017\",atr_filter=0.02, save_path=\"signal result atr\", reverse=1)\n",
    "\n",
    "    \n",
    "# reverse=1\n",
    "# reverse=1\n",
    "# reverse=1\n",
    "# reverse=1\n",
    "# reverse=1\n",
    "# Wall time: 28min 52s\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600519 train sharpe  nan test sharpe  nan\n",
      "000858 train sharpe  nan test sharpe  nan\n",
      "000568 train sharpe  nan test sharpe  nan\n",
      "600809 train sharpe  nan test sharpe  nan\n",
      "002304 train sharpe  0.26985728009763355 test sharpe  0.09326742063043855\n"
     ]
    }
   ],
   "source": [
    "for product in product_list:\n",
    "    signal_stat = load(SAVE_PATH+\"/signal result atr/\"+product+\".\"+\"wine.ensemble.lasso.4096.trend\"+\".pkl\")\n",
    "    train_stat = signal_stat[\"train.stat\"]\n",
    "    good_strat = (train_stat[\"final.result\"][\"avg.ret\"]>0.001)\n",
    "    train_pnl = train_stat[\"daily.pnl\"][:, good_strat].sum(axis=1)/sum(good_strat)\n",
    "    test_stat = signal_stat[\"test.stat\"]\n",
    "    test_pnl = test_stat[\"daily.pnl\"][:, good_strat].sum(axis=1)/sum(good_strat)\n",
    "    print(product, \"train sharpe \", sharpe(train_pnl), \"test sharpe \", sharpe(test_pnl))\n",
    "\n",
    "\n",
    "# 600519 train sharpe  nan test sharpe  nan\n",
    "# 000858 train sharpe  nan test sharpe  nan\n",
    "# 000568 train sharpe  nan test sharpe  nan\n",
    "# 600809 train sharpe  nan test sharpe  nan\n",
    "# 002304 train sharpe  0.22738178582067017 test sharpe  -0.03630524094131039\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- it's not as good as thre model in week 9\n",
    "- maybe we can use another way of ensemble\n",
    "- we can generate 10 separate models with each pnl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2nd Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coefficients of all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate 10 different models, and save their coefficients rather than calcualte their mean\n",
    "nfold = 10\n",
    "model_coef = dict([])\n",
    "for i_mod in range(n_mod):\n",
    "    train_std_mat = dict([])\n",
    "    model_coef[str(i_mod)] = dict([])\n",
    "    x_train = np.zeros((0, n_signal))\n",
    "    y_train = np.array([])\n",
    "    for product in product_list:\n",
    "        cur_mat = all_train_array[product][i_mod,:,:]\n",
    "        train_std_mat[product] = np.std(cur_mat, axis=0)\n",
    "        x_train = np.vstack((x_train, cur_mat[:,:n_signal]/train_std_mat[product][:n_signal]))\n",
    "        y_train = np.concatenate((y_train, cur_mat[:,n_signal]/train_std_mat[product][n_signal]))\n",
    "    scaler =  StandardScaler(copy=True, with_mean=False, with_std=True)\n",
    "    scaler.fit(x_train)\n",
    "    x_std = np.sqrt(scaler.var_)\n",
    "    x_train_normal = scaler.transform(x_train)\n",
    "    model = LassoCV(n_alphas=100, fit_intercept=False, cv=10, max_iter=10000).fit(x_train_normal, y_train)\n",
    "    fit_coef = model.coef_/x_std\n",
    "    for product in product_list:\n",
    "        coef = fit_coef*train_std_mat[product][n_signal]/train_std_mat[product][:n_signal]\n",
    "        model_coef[str(i_mod)][product] = coef\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(model_coef, SAVE_PATH+\"wine.ensemble.all.coef.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "reverse=1\n",
      "reverse=1\n",
      "reverse=1\n",
      "reverse=1\n",
      "reverse=1\n",
      "1\n",
      "reverse=1\n",
      "reverse=1\n",
      "reverse=1\n",
      "reverse=1\n",
      "reverse=1\n",
      "2\n",
      "reverse=1\n",
      "reverse=1\n",
      "reverse=1\n",
      "reverse=1\n",
      "reverse=1\n",
      "3\n",
      "reverse=1\n",
      "reverse=1\n",
      "reverse=1\n",
      "reverse=1\n",
      "reverse=1\n",
      "4\n",
      "reverse=1\n",
      "reverse=1\n",
      "reverse=1\n",
      "reverse=1\n",
      "reverse=1\n",
      "5\n",
      "reverse=1\n",
      "reverse=1\n",
      "reverse=1\n",
      "reverse=1\n",
      "reverse=1\n",
      "6\n",
      "reverse=1\n",
      "reverse=1\n",
      "reverse=1\n",
      "reverse=1\n",
      "reverse=1\n",
      "7\n",
      "reverse=1\n",
      "reverse=1\n",
      "reverse=1\n",
      "reverse=1\n",
      "reverse=1\n",
      "8\n",
      "reverse=1\n",
      "reverse=1\n",
      "reverse=1\n",
      "reverse=1\n",
      "reverse=1\n",
      "9\n",
      "reverse=1\n",
      "reverse=1\n",
      "reverse=1\n",
      "reverse=1\n",
      "reverse=1\n"
     ]
    }
   ],
   "source": [
    "for i_mod in range(n_mod):\n",
    "    print(i_mod)\n",
    "    strat = \"wine.ensemble.lasso.\"+str(i_mod)+\".\"+str(period)\n",
    "    for product in product_list:\n",
    "        os.makedirs(SAVE_PATH+\"/tmp pkl/\"+product+\"/\"+strat, exist_ok=True)\n",
    "        coef = model_coef[str(i_mod)][product]\n",
    "        parLapply(CORE_NUM, all_dates, par_get_daily_pred, \n",
    "                  product=product, coef=coef, strat=strat, HEAD_PATH=SAVE_PATH, SAVE_PATH=SAVE_PATH)\n",
    "        par_get_all_signal(strat, all_dates, product, 4096, SAVE_PATH=SAVE_PATH)    \n",
    "        evaluate_signal(\"wine.ensemble.lasso.\"+str(i_mod), all_dates, product, 0.001, 20, CORE_NUM, SAVE_PATH, SAVE_PATH,\n",
    "                            period=4096, split_str=\"2017\",atr_filter=0.02, save_path=\"signal result atr\", reverse=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600519 train sharpe  0.12703716704204124 test sharpe  -0.07853609641183407\n",
      "000858 train sharpe  0.48411162192347634 test sharpe  -1.0232483096300733\n",
      "000568 train sharpe  nan test sharpe  nan\n",
      "600809 train sharpe  nan test sharpe  nan\n",
      "002304 train sharpe  0.2804429717513608 test sharpe  0.05867656544930187\n"
     ]
    }
   ],
   "source": [
    "min_pnl = 2\n",
    "min_num = 10\n",
    "for product in product_list:\n",
    "    i = 0\n",
    "    for i_mod in range(n_mod):\n",
    "        strat = \"wine.ensemble.lasso.\"+str(i_mod)+\".\"+str(period)\n",
    "        signal_stat = load(SAVE_PATH+\"/signal result atr/\"+product+\".\"+strat+\".trend.pkl\")\n",
    "        train_stat = signal_stat[\"train.stat\"]\n",
    "        good_strat = (train_stat[\"final.result\"][\"avg.ret\"]>0.001)\n",
    "        if sum(good_strat)==0:\n",
    "            continue\n",
    "        train_pnl = train_stat[\"daily.pnl\"][:, good_strat].sum(axis=1)/sum(good_strat)\n",
    "        test_stat = signal_stat[\"test.stat\"]\n",
    "        test_pnl = test_stat[\"daily.pnl\"][:, good_strat].sum(axis=1)/sum(good_strat)\n",
    "        if i==0:\n",
    "            test_all_pnl = np.zeros([len(test_pnl), n_mod])\n",
    "            train_all_pnl = np.zeros([len(train_pnl), n_mod])\n",
    "        test_all_pnl[:,i] = test_pnl\n",
    "        train_all_pnl[:,i] = train_pnl\n",
    "        i = i+1\n",
    "    train_portfolio = np.array(np.mean(train_all_pnl[:,:i], axis=1))\n",
    "    test_portfolio = np.array(np.mean(test_all_pnl[:,:i], axis=1))\n",
    "    print(product, \"train sharpe \", sharpe(train_portfolio), \"test sharpe \", sharpe(test_portfolio))\n",
    "    \n",
    "\n",
    "# 600519 train sharpe  0.17117304776678074 test sharpe  0.2755301956905567\n",
    "# 000858 train sharpe  0.46397312403353136 test sharpe  -1.0265335177621764\n",
    "# 000568 train sharpe  nan test sharpe  nan\n",
    "# 600809 train sharpe  nan test sharpe  nan\n",
    "# 002304 train sharpe  0.2836953725557839 test sharpe  0.0834417960846937\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we can see that the test result accpetable even though there is a small drawback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
