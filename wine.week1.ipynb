{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev2 toc-item\"><a href=\"#First-Example-(Never-use-it-in-trading)\" data-toc-modified-id=\"First-Example-(Never-use-it-in-trading)-01\"><span class=\"toc-item-num\">0.1&nbsp;&nbsp;</span>First Example (Never use it in trading)</a></div><div class=\"lev2 toc-item\"><a href=\"#Compute-Daily-PnL\" data-toc-modified-id=\"Compute-Daily-PnL-02\"><span class=\"toc-item-num\">0.2&nbsp;&nbsp;</span>Compute Daily PnL</a></div><div class=\"lev3 toc-item\"><a href=\"#Repeat-with-threshold-0.01\" data-toc-modified-id=\"Repeat-with-threshold-0.01-021\"><span class=\"toc-item-num\">0.2.1&nbsp;&nbsp;</span>Repeat with threshold 0.01</a></div><div class=\"lev3 toc-item\"><a href=\"#Repeat-with-Threshold-0.02\" data-toc-modified-id=\"Repeat-with-Threshold-0.02-022\"><span class=\"toc-item-num\">0.2.2&nbsp;&nbsp;</span>Repeat with Threshold 0.02</a></div><div class=\"lev2 toc-item\"><a href=\"#Compute-Daily-PnL-with-A-Different-Schema\" data-toc-modified-id=\"Compute-Daily-PnL-with-A-Different-Schema-03\"><span class=\"toc-item-num\">0.3&nbsp;&nbsp;</span>Compute Daily PnL with A Different Schema</a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEAD_PATH = './'\n",
    "DATA_PATH = HEAD_PATH + \"data/stocks/\"\n",
    "SAVE_PATH = HEAD_PATH + \"ckpt/\"\n",
    "TEMP_PATH = SAVE_PATH + \"tmp pkl/\"\n",
    "OUTPUT_PATH = HEAD_PATH + 'output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    " \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- path of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Welcome to Quantitative and High Frequency Trading Training Program\n",
    "- Let's start with basic data analysis with python\n",
    "- you can use Shift+Enter to run each line of code\n",
    "\n",
    "- path of our program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA_PATH = HEAD_PATH + \"/stock_pkl/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- set working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.chdir(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- number of cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CORE_NUM = int(os.environ['NUMBER_OF_PROCESSORS'])\n",
    "CORE_NUM = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- get working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sample product, you can change to your specific product symbol\n",
    "- for this quarter we analyze all of the black medals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_list = [\"600519\", \"000858\", \"000568\", \"600809\", \"002304\"]\n",
    "product = product_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- directory of the product's data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dire = DATA_PATH + product\n",
    "dire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get all available data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dates = list(map(lambda x: x,os.listdir(DATA_PATH + product)))\n",
    "len(all_dates)\n",
    "## 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we can see that there are 974 days\n",
    "- all of the products have the same trading days\n",
    "- so we just take a as an example\n",
    "- get first several dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- specify a date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = \"2018\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- loading data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import _pickle as cPickle\n",
    "import gzip\n",
    "\n",
    "with gzip.open(dire+\"/\"+date+\".pkl\", 'rb', compresslevel=1) as file_object:\n",
    "    raw_data = file_object.read()\n",
    "data = cPickle.loads(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(3).to_csv(\"heha.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get the column names of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns\n",
    "\n",
    "#Index(['date.time', 'price', 'traded.num', 'turnover', 'qty', 'bid1', 'bid2',\n",
    "#       'bid3', 'bid4', 'bid5', 'ask1', 'ask2', 'ask3', 'ask4', 'ask5',\n",
    "#       'bid1.qty', 'bid2.qty', 'bid3.qty', 'bid4.qty', 'bid5.qty', 'ask1.qty',\n",
    "#       'ask2.qty', 'ask3.qty', 'ask4.qty', 'ask5.qty', 'adjust'],\n",
    "#      dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(3).to_csv(\"head3_2018.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[0:10,0:10]\n",
    "\n",
    "#  \tdate.time \tprice \ttraded.num \tturnover \tqty \tbid1 \tbid2 \tbid3 \tbid4 \tbid5\n",
    "# 0 \t2018-01-02 09:25:01 \t700.00 \t152 \t22776600.0 \t325 \t700.00 \t699.99 \t699.98 \t699.90 \t699.80\n",
    "# 1 \t2018-01-02 09:30:00 \t701.29 \t25 \t23459540.0 \t335 \t700.90 \t700.01 \t700.00 \t699.99 \t699.98\n",
    "# 2 \t2018-01-02 09:30:02 \t701.30 \t56 \t7309588.0 \t104 \t701.30 \t701.10 \t701.00 \t700.99 \t700.90\n",
    "# 3 \t2018-01-02 09:30:06 \t702.00 \t34 \t3897012.0 \t56 \t702.00 \t701.89 \t701.59 \t701.48 \t701.30\n",
    "# 4 \t2018-01-02 09:30:09 \t703.00 \t7 \t1897088.0 \t27 \t702.25 \t702.00 \t701.99 \t701.49 \t701.48\n",
    "# 5 \t2018-01-02 09:30:12 \t702.50 \t4 \t351200.0 \t5 \t702.50 \t702.25 \t702.00 \t701.99 \t701.90\n",
    "# 6 \t2018-01-02 09:30:15 \t703.00 \t6 \t421588.0 \t6 \t702.90 \t702.50 \t702.43 \t702.00 \t701.99\n",
    "# 7 \t2018-01-02 09:30:18 \t703.00 \t10 \t1335568.0 \t19 \t702.90 \t702.50 \t702.43 \t702.00 \t701.99\n",
    "# 8 \t2018-01-02 09:30:21 \t703.05 \t7 \t729680.0 \t10 \t703.05 \t703.01 \t703.00 \t702.90 \t702.50\n",
    "# 9 \t2018-01-02 09:30:24 \t703.00 \t15 \t1132036.0 \t16 \t703.05 \t703.00 \t702.99 \t702.90 \t702.54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[0:10,10:20]\n",
    "\n",
    "# ask1 \task2 \task3 \task4 \task5 \tbid1.qty \tbid2.qty \tbid3.qty \tbid4.qty \tbid5.qty\n",
    "# 0 \t700.04 \t701.00 \t701.30 \t701.47 \t702.00 \t84 \t5 \t1 \t7 \t5\n",
    "# 1 \t701.29 \t701.80 \t701.90 \t702.00 \t702.18 \t1 \t46 \t100 \t5 \t1\n",
    "# 2 \t701.89 \t703.00 \t703.43 \t703.97 \t704.00 \t95 \t1 \t47 \t1 \t1\n",
    "# 3 \t702.50 \t702.88 \t703.00 \t703.28 \t703.43 \t108 \t3 \t1 \t1 \t124\n",
    "# 4 \t703.00 \t703.28 \t703.43 \t703.50 \t703.90 \t1 \t110 \t1 \t1 \t1\n",
    "# 5 \t702.90 \t703.00 \t703.28 \t703.43 \t703.50 \t34 \t3 \t108 \t1 \t4\n",
    "# 6 \t703.00 \t703.28 \t703.43 \t703.50 \t703.90 \t1 \t35 \t7 \t108 \t1\n",
    "# 7 \t702.99 \t703.00 \t703.28 \t703.43 \t703.50 \t136 \t35 \t7 \t109 \t1\n",
    "# 8 \t703.28 \t703.43 \t703.50 \t703.90 \t703.97 \t125 \t1 \t2 \t2 \t35\n",
    "# 9 \t703.90 \t703.97 \t704.00 \t704.18 \t704.43 \t3 \t4 \t1 \t1 \t2\n",
    "\n",
    "# # data.iloc[0:10,20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[0:10,20:]\n",
    "\n",
    "# ask1.qty \task2.qty \task3.qty \task4.qty \task5.qty \tadjust\n",
    "# 0 \t1 \t2 \t50 \t1 \t3 \t1.530971\n",
    "# 1 \t5 \t1 \t11 \t3 \t1 \t1.530971\n",
    "# 2 \t4 \t28 \t1 \t1 \t31 \t1.530971\n",
    "# 3 \t20 \t1 \t21 \t1 \t1 \t1.530971\n",
    "# 4 \t15 \t1 \t1 \t2 \t8 \t1.530971\n",
    "# 5 \t1 \t15 \t1 \t1 \t2 \t1.530971\n",
    "# 6 \t14 \t1 \t1 \t2 \t8 \t1.530971\n",
    "# 7 \t1 \t4 \t1 \t1 \t2 \t1.530971\n",
    "# 8 \t1 \t1 \t2 \t8 \t1 \t1.530971\n",
    "# 9 \t8 \t1 \t32 \t1 \t12 \t1.530971"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- plot the price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install plotly\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(1, figsize=(16, 10))\n",
    "plt.plot(data[\"price\"].values);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(16, 10))\n",
    "plt.plot(data[\"price\"]*data[\"adjust\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"date.time\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- plot price change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(path):\n",
    "    with gzip.open(path, 'rb', compresslevel=1) as file_object:\n",
    "        raw_data = file_object.read()\n",
    "    return cPickle.loads(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import compute, delayed\n",
    "def parLapply(CORE_NUM, iterable, func, *args, **kwargs):\n",
    "    with dask.config.set(scheduler='processes', num_workers=CORE_NUM):\n",
    "        f_par = functools.partial(func, *args, **kwargs)\n",
    "        result = compute([delayed(f_par)(item) for item in iterable])[0]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(data, path):\n",
    "    serialized = cPickle.dumps(data)\n",
    "    with gzip.open(path, 'wb', compresslevel=1) as file_object:\n",
    "        file_object.write(serialized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addWpr(date, product):\n",
    "    data = load(DATA_PATH+\"/\"+product+\"/\"+date)\n",
    "    data[\"wpr\"] = (data[\"bid1\"]*data[\"ask1.qty\"]+data[\"ask1\"]*data[\"bid1.qty\"])/(data[\"bid1.qty\"]+data[\"ask1.qty\"])\n",
    "    outlier = (data[\"bid1\"]<1e-6) | (data[\"ask1\"]<1e-6) | (np.isnan(data[\"wpr\"]))\n",
    "    data[\"wpr\"][outlier] = data[\"price\"][outlier]\n",
    "    data[\"next.bid\"] = data[\"bid1\"].shift(-1)\n",
    "    data[\"next.ask\"] = data[\"ask1\"].shift(-1)\n",
    "    data[\"wpr\"] = data[\"wpr\"]*data[\"adjust\"]\n",
    "    data[\"wpr.ret\"] = data[\"wpr\"]-data[\"wpr\"].shift(1)\n",
    "    data[\"wpr.ret\"][0] = 0\n",
    "    data[\"ret\"] = np.log(data[\"wpr\"]) - np.log(data[\"wpr\"]).shift(1)\n",
    "    data[\"ret\"][0] = 0\n",
    "    save(data, DATA_PATH+\"/\"+product+\"/\"+date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORE_NUM = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for product in product_list:\n",
    "    result = parLapply(CORE_NUM, sorted(all_dates), addWpr, product=product)\n",
    ";\n",
    "\n",
    "## Wall time: 1min 23s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product = product_list[0]\n",
    "data = load(DATA_PATH+\"/\"+product+\"/\"+\"2018.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(16, 10))\n",
    "plt.title(date)\n",
    "data[\"wpr.ret\"].values[0]=0\n",
    "plt.plot(data[\"wpr.ret\"].values);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- plot price log return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(16, 10))\n",
    "plt.title(date)\n",
    "plt.plot(data[\"ret\"].values);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- count the number of rising ticks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(data['ret']>0)/len(data)\n",
    "## 0.4818650666836557"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- count the number of falling ticks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(data['ret']<0)/len(data)\n",
    "## 0.4854687276826312"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(data['ret']==0)/len(data)\n",
    "## 0.032666205633713086"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- count the number of total ticks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)\n",
    "## 973177"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- formula to calculate wpr:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wpr = (data[\"bid1\"]*data[\"ask1.qty\"]+data[\"ask1\"]*data[\"bid1.qty\"])/(data[\"bid1.qty\"]+data[\"ask1.qty\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- upper limit or lower limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = (data[\"ask1\"]<1e-6) | (data[\"bid1\"]<1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in this case, one of bid and ask price is zero, so using newest price as wpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wpr[limit] = data[\"price\"][limit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wpr = wpr*data[\"adjust\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wpr.head()\n",
    "\n",
    "# 0    1071.740430\n",
    "# 1    1073.157299\n",
    "# 2    1074.536951\n",
    "# 3    1075.387733\n",
    "# 4    1075.196361\n",
    "# dtype: float64\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data[\"wpr\"].head()\n",
    "\n",
    "# 0    1071.740430\n",
    "# 1    1073.157299\n",
    "# 2    1074.536951\n",
    "# 3    1075.387733\n",
    "# 4    1075.196361\n",
    "# Name: wpr, dtype: float64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(abs(wpr - data[\"wpr\"]) > 0.00000001)\n",
    "## 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- stationary test for return series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- load related package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.tsa.stattools as ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ts.adfuller(data[\"ret\"], maxlag=int(pow(len(data[\"ret\"])-1,(1/3))), regression='ct', autolag=None)\n",
    "result\n",
    "\n",
    "# (-98.75770074233702,\n",
    "#  0.0,\n",
    "#  99,\n",
    "#  973077,\n",
    "#  {'1%': -3.958779303610315,\n",
    "#   '5%': -3.4104945118827037,\n",
    "#   '10%': -3.127052657142275})\n",
    "\n",
    "## for this test the smaller the value the more stationary it is\n",
    "## so it's stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ts.kpss(data[\"ret\"], regression='c', lags=int(3*math.sqrt(len(data[\"ret\"]))))\n",
    "result\n",
    "                 \n",
    "# (0.0805796450417109,\n",
    "#  0.1,\n",
    "#  2959,\n",
    "#  {'10%': 0.347, '5%': 0.463, '2.5%': 0.574, '1%': 0.739})                                                 \n",
    "                                                                   \n",
    "                                                                    \n",
    "\n",
    "    ## for KPSS test, small value means level stationary, large value means trend\n",
    "    ## its p-value is 0.1 so it's stationary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get price movements of 120 ticks, it's about 1 minute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- plot the price movements of 120 ticks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the looking back 120 days sum, reset index\n",
    "def use_D_day_freq_ret(data, period=120):\n",
    "    temp =  (data[\"ret\"].rolling(period).sum()).dropna().reset_index(drop=True)\n",
    "    selected_no_overlap = range(1, len(temp), period)\n",
    "    return temp.iloc[selected_no_overlap]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_data = use_D_day_freq_ret(data, 120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- test whether ret.120 is stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = ts.adfuller(selected_data, maxlag=int(pow(len(selected_data)-1,(1/3))), regression='ct', autolag=None)\n",
    "result\n",
    "\n",
    "# (-17.720699767515875,\n",
    "#  0.0,\n",
    "#  20,\n",
    "#  8088,\n",
    "#  {'1%': -3.95988975975366,\n",
    "#   '5%': -3.411032967099939,\n",
    "#   '10%': -3.1273697435248473})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ts.kpss(selected_data, regression='c', lags=int(3*math.sqrt(len(selected_data))/13))\n",
    "result\n",
    "\n",
    "# (0.08180242830810695,\n",
    "#  0.1,\n",
    "#  20,\n",
    "#  {'10%': 0.347, '5%': 0.463, '2.5%': 0.574, '1%': 0.739})\n",
    "\n",
    "## it's still stationary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we can check longer period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_D_day_freq_ret(data, 2000).shape\n",
    "\n",
    "## 486"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So there are too few points to test for stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import _pickle as cPickle\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- there may be too few data\n",
    "- we can check for more days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_ret_v2(date, period):\n",
    "    data = load(DATA_PATH + product+\"/\"+date)\n",
    "    return use_D_day_freq_ret(data, period)\n",
    "\n",
    "def flat_nested_list(high_dimension_array):\n",
    "    from itertools import chain \n",
    "    return list(chain.from_iterable(high_dimension_array)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(all_dates))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data, and select period 4096, append them to a list call it result\n",
    "# result[0] is numpy.series type\n",
    "all_dates = sorted(all_dates)\n",
    "result = parLapply(CORE_NUM, sorted(all_dates), get_sample_ret_v2, period=4096)\n",
    "# concat all the 11 years data into one long list\n",
    "ret_long = flat_nested_list(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(16, 10))\n",
    "plt.title(\"all return_long\")\n",
    "plt.plot(ret_long);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ts.adfuller(ret_long, maxlag=int(pow(len(ret_long)-1,(1/3))), regression='ct', autolag=None)\n",
    "result\n",
    "\n",
    "# (-11.956867152061653,\n",
    "#  4.513188081085745e-19,\n",
    "#  12,\n",
    "#  1959,\n",
    "#  {'1%': -3.963398711813558,\n",
    "#   '5%': -3.412733504022824,\n",
    "#   '10%': -3.128370882799846})\n",
    "\n",
    "## it's still stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ts.kpss(ret_long, regression='c', lags=int(3*math.sqrt(len(ret_long))/13))\n",
    "result\n",
    "\n",
    "# (0.21304537820127364,\n",
    "#  0.1,\n",
    "#  10,\n",
    "#  {'10%': 0.347, '5%': 0.463, '2.5%': 0.574, '1%': 0.739})\n",
    "\n",
    "# ## p-value is 0.1 so it's stationary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we can see that, it's still stationary for 4096 ticks\n",
    "\n",
    "\n",
    "- check simple strategy\n",
    "\n",
    "\n",
    "- set triger threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.001\n",
    "len(ret_long)\n",
    "# 1972"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- make up the return series to match uo with original data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- plot price series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- number of ticks above the threshold, to be long position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(np.array(ret_long)>threshold)\n",
    "## 957"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- number of ticks below negetive threshold, to be short position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(np.array(ret_long)< -threshold)\n",
    "## 910"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- set position\n",
    "\n",
    "\n",
    "- number of ticks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Example (Never use it in trading)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- set position just the sanme as signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(dire+\"/2011.pkl\", 'rb', compresslevel=1) as file_object:\n",
    "    raw_data = file_object.read()\n",
    "data = cPickle.loads(raw_data) ## original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "period = 4096\n",
    "n_bar = len(data)  ## number of bars\n",
    "unit = np.std(data[\"ret\"]) ## standard deviation of return\n",
    "np.random.seed(10)\n",
    "##  we repeat the above code to get daily result\n",
    "ret_long = (data[\"ret\"].rolling(period).sum()).dropna().reset_index(drop=True) ## future return, used as signal—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_long = ret_long.append(pd.Series([0]*(len(data)-len(ret_long)))).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal = pd.Series([0] * n_bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal[(ret_long>threshold) & (np.array(data[\"next.ask\"])>0)] = 1 ## signal>thre, position =1\n",
    "signal[(ret_long< -threshold) & (np.array(data[\"next.bid\"])>0)] = -1 ## signal< -thre, position = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Daily PnL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get_daily_pnl_fast, load the original data, each time read in one product, one year --> data\n",
    "- compute 4096 (period) rolling, drop first few data, then complete it with zeros appended to the tail of 4096 rolling data \n",
    "- Whenever, the rolling > threshold, and the next.ask > 0, we set is as position = 1 (as signal)\n",
    "- Whenever, the rolling < -threshold, and the next.bid > 0, we set is as position = -1 (as signal)\n",
    "- Let position = signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## backtest method: signal value decides position\n",
    "from collections import OrderedDict\n",
    "def get_daily_pnl_fast(date, product=\"rb\", period=4096, threshold=0.001, buy_tranct=1.5e-4, sell_tranct=11.5e-4, noise=0):\n",
    "    with gzip.open(dire+\"/\"+date, 'rb', compresslevel=1) as file_object:\n",
    "        raw_data = file_object.read()\n",
    "    data = cPickle.loads(raw_data) ## original data\n",
    "    n_bar = len(data)  ## number of bars\n",
    "    unit = np.std(data[\"ret\"]) ## standard deviation of return\n",
    "    np.random.seed(10)\n",
    "    ##  we repeat the above code to get daily result\n",
    "    ret_long = (data[\"ret\"].rolling(period).sum()).dropna().reset_index(drop=True) ## future return, used as signal\n",
    "    ret_long = ret_long.append(pd.Series([0]*(len(data)-len(ret_long)))).reset_index(drop=True)\n",
    "    signal = pd.Series([0] * n_bar)\n",
    "    signal[(ret_long>threshold) & (np.array(data[\"next.ask\"])>0)] = 1 ## signal>thre, position =1\n",
    "    signal[(ret_long< -threshold) & (np.array(data[\"next.bid\"])>0)] = -1 ## signal< -thre, position = -1\n",
    "    # Note, we set position = signal\n",
    "    position=signal\n",
    "    position[0]=0\n",
    "    position[n_bar-1] = 0 ## close position before the end of day\n",
    "    position[n_bar-2] = 0\n",
    "    change_pos = position - position.shift(1)\n",
    "    change_pos[0] = 0\n",
    "    \n",
    "    # base ? all 0s\n",
    "    change_base = np.zeros(n_bar)\n",
    "    change_buy = np.array(change_pos>0)\n",
    "    change_sell = np.array(change_pos<0)\n",
    "    # We need to add the set the price of buy, the price of sell properly; meaning, we need to add fee.\n",
    "    change_base[change_buy] = data[\"next.ask\"][change_buy]*(1+buy_tranct) ## buy price, use next ask, tranct cost use notional*ratio\n",
    "    change_base[change_sell] = data[\"next.bid\"][change_sell]*(1-sell_tranct) ## sell price use next bid\n",
    "    # \n",
    "    raw_pnl = -(change_base*change_pos).cumsum()+position*data[\"price\"]\n",
    "    date = np.array([x[0:10] for x in data[\"date.time\"]])\n",
    "    next_date = np.append(date[1:],'1')\n",
    "    end_day = date!=next_date\n",
    "    final_pnl = -sum(change_base*change_pos) ## total pnl, there is a negative sign, because selling get money and buying pay money\n",
    "    turnover = sum(change_base*abs(change_pos))\n",
    "    num = sum((position!=0) & (change_pos!=0)) ## number of trades\n",
    "    hld_period = sum(position!=0)   ## holding period\n",
    "    daily_pnl = raw_pnl[end_day].reset_index(drop=True)\n",
    "    pnl = np.append(daily_pnl[0], np.diff(daily_pnl))\n",
    "    ## finally we combine the statistics into a data frame\n",
    "    #result = pd.DataFrame({\"final.pnl\": final_pnl, \"turnover\": turnover, \"num\": num, \"hld.period\": hld_period}, index=[0])\n",
    "    #result = {\"date\": date, \"final.pnl\": final_pnl, \"turnover\": turnover, \"num\": num, \"hld.period\": hld_period}\n",
    "    result = OrderedDict([(\"date\", date[end_day]), (\"pnl\", pnl),\n",
    "                          (\"final.pnl\", final_pnl), (\"turnover\", turnover), (\"num\", num), (\"hld.period\", hld_period)])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we try one day as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dire+\"/\"+all_dates[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Then we use it for all days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import functools\n",
    "with dask.config.set(scheduler='processes', num_workers=1):\n",
    "    f_par = functools.partial(get_daily_pnl_fast, product=product_list[0], period=4096, threshold=0.001, noise=0)\n",
    "result = compute([delayed(f_par)(date) for date in all_dates])[0]\n",
    "    \n",
    "# Wall time: 35.2 s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we can use a function to check its performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "def get_performance(result, spread=1):\n",
    "    '''\n",
    "    Concat all the pnl and plot dates vs cumulative pnl, also compute other metrics and return\n",
    "    '''\n",
    "    date = []\n",
    "    pnl = []\n",
    "    num = 0\n",
    "    hld = 0\n",
    "    # Iterate through all the years\n",
    "    for i in range(len(result)):\n",
    "        date.append(result[i][\"date\"])\n",
    "        pnl.append(result[i][\"pnl\"])\n",
    "        num += result[i][\"num\"]\n",
    "        hld += result[i][\"hld.period\"]\n",
    "        \n",
    "    date = np.concatenate(date)\n",
    "    pnl = np.concatenate(pnl)\n",
    "    \n",
    "    date_format = [pd.to_datetime(d) for d in date]\n",
    "    plt.figure(1, figsize=(16, 10))\n",
    "    plt.title(\"\")\n",
    "    plt.xlabel(\"date\")\n",
    "    plt.ylabel(\"cumulative pnl\")\n",
    "    plt.plot(date_format, pnl.cumsum())\n",
    "    n_days = len(date)\n",
    "    if num==0:\n",
    "        return;\n",
    "    if (pnl.std()==0):\n",
    "        sharpe = 0\n",
    "    else:\n",
    "        sharpe = pnl.mean()/pnl.std()*math.sqrt(250)\n",
    "    pnl_pd = pd.Series(pnl.cumsum())\n",
    "    drawdown = max(pnl_pd.cummax()-pnl_pd)/pnl_pd.iloc[-1]\n",
    "    mar = 1/drawdown\n",
    "    win_ratio = sum(pnl>0)/n_days\n",
    "   \n",
    "    avg_pnl = sum(pnl)/num\n",
    "    hld_period = hld/num\n",
    "    return OrderedDict([(\"sharpe\", sharpe), (\"drawdown\", drawdown), (\"mar\", mar), (\"win.ratio\", win_ratio)\n",
    "                        , (\"num\", num/n_days), (\"avg.pnl\", avg_pnl), (\"hld.period\", hld_period)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_temp = []\n",
    "for i in result:\n",
    "    #print(i)\n",
    "    #for key, val in i.items():\n",
    "    #    print(key, type(val))\n",
    "    temp = pd.DataFrame(i)\n",
    "    final_temp.append(temp)\n",
    "temp = pd.concat(final_temp)\n",
    "temp.to_csv(\"my_check2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(get_performance(result, 1), index=[0])\n",
    "\n",
    "#  \tsharpe \tdrawdown \tmar \twin.ratio \tnum \tavg.pnl \thld.period\n",
    "# \t-5.807688 \t-0.999908 \t-1.000092 \t0.308607 \t21.703689 \t-0.732229 \t144.721944\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we can see it's really bad\n",
    "- we can try increasing threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat with threshold 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import functools\n",
    "with dask.config.set(scheduler='processes', num_workers=CORE_NUM):\n",
    "    f_par = functools.partial(get_daily_pnl_fast, product=product_list[0], period=4096, threshold=0.01,  noise=0)\n",
    "result_2 = compute([delayed(f_par)(date) for date in all_dates])[0]\n",
    "\n",
    "#Wall time: 35.4 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(get_performance(result_2), index=[0])\n",
    "\n",
    "\n",
    "# \tsharpe \tdrawdown \tmar \twin.ratio \tnum \tavg.pnl \thld.period\n",
    "# -6.773244 \t-0.999765 \t-1.000235 \t0.215574 \t16.952459 \t-0.659888 \t110.695049"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat with Threshold 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import functools\n",
    "with dask.config.set(scheduler='processes', num_workers=CORE_NUM):\n",
    "    f_par = functools.partial(get_daily_pnl_fast, product=product_list[0], period=4096, threshold=0.02, noise=0)\n",
    "result_3 = compute([delayed(f_par)(date) for date in all_dates])[0]\n",
    "\n",
    "## Wall time: 35.2 s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(get_performance(result_3, 1), index=[0])\n",
    "\n",
    "#  \tsharpe \tdrawdown \tmar \twin.ratio \tnum \tavg.pnl \thld.period\n",
    "# \t-5.034499 \t-0.999573 \t-1.000427 \t0.168443 \t10.339754 \t-0.594999 \t92.146419"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Daily PnL with A Different Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Although it's profitable there are very few trades.\n",
    "- Now we use a different scheme.\n",
    "- In previous scheme, we close our position when the value is not strong enough.\n",
    "- It may close the positions too soon that it cannot cover transaction cost on average\n",
    "- So we change our backtest method to make it holding positions longer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## backtest use good method\n",
    "from collections import OrderedDict\n",
    "def get_daily_pnl(date, product=\"rb\", period=2000, threshold=0.001, buy_tranct=1.5e-4, sell_tranct=11.5e-4, noise=0):\n",
    "    with gzip.open(dire+\"/\"+date, 'rb', compresslevel=1) as file_object:\n",
    "        raw_data = file_object.read()\n",
    "    data = cPickle.loads(raw_data)\n",
    "    n_bar = len(data)\n",
    "    unit = np.std(data[\"ret\"])\n",
    "    np.random.seed(10)\n",
    "    # here noise = 0\n",
    "    noise_ret = np.random.normal(scale=unit*noise, size=n_bar)\n",
    "    ##  we repeat the above code to get daily result\n",
    "    ret_2000 = (data[\"ret\"].rolling(period).sum()).dropna().reset_index(drop=True)\n",
    "    ret_2000 = ret_2000.append(pd.Series([0]*(len(data)-len(ret_2000)))).reset_index(drop=True) + noise_ret\n",
    "    signal = pd.Series([0] * n_bar)\n",
    "    signal[ret_2000 > threshold] = 1 #\n",
    "    signal[ret_2000 < -threshold] = -1\n",
    "    position_pos = pd.Series([np.nan] * n_bar)\n",
    "    position_pos[0] = 0\n",
    "    position_pos[(signal==1) & (data[\"next.ask\"]>0) & (data[\"next.bid\"]>0)] = 1## if signal==1, position_pos=1\n",
    "    position_pos[(ret_2000 < -threshold) & (data[\"next.bid\"]>0)] = 0  ## if ret< -threshold, position_pos=0\n",
    "    position_pos.ffill(inplace=True)\n",
    "    \n",
    "    position_neg = pd.Series([np.nan] * n_bar)\n",
    "    position_neg[0] = 0\n",
    "    position_neg[(signal==-1) & (data[\"next.ask\"]>0) & (data[\"next.bid\"]>0)] = -1 ## if signal==-1, position_neg=-1\n",
    "    position_neg[(ret_2000> threshold) & (data[\"next.ask\"]>0)] = 0 ## if ret> threshold, position_neg=0\n",
    "    position_neg.ffill(inplace=True)\n",
    "    \n",
    "    position = position_pos + position_neg ## total position\n",
    "    position[0]=0\n",
    "    position[n_bar-1] = 0\n",
    "    position[n_bar-2] = 0\n",
    "    \n",
    "    change_pos = position - position.shift(1)\n",
    "    change_pos[0] = 0\n",
    "    \n",
    "    change_base = pd.Series([0] * n_bar)\n",
    "    \n",
    "    change_buy = change_pos > 0\n",
    "    change_sell = change_pos < 0\n",
    "    change_base[change_buy] = data[\"next.ask\"][change_buy]*(1+buy_tranct)\n",
    "    change_base[change_sell] = data[\"next.bid\"][change_sell]*(1-sell_tranct)\n",
    "    raw_pnl = -(change_base*change_pos).cumsum()+position*data[\"price\"]\n",
    "    date = np.array([x[0:10] for x in data[\"date.time\"]])\n",
    "    next_date = np.append(date[1:],'1')\n",
    "    end_day = date!=next_date\n",
    "    final_pnl = -sum(change_base*change_pos) ## total pnl, there is a negative sign, because selling get money and buying pay money\n",
    "    turnover = sum(change_base*abs(change_pos))\n",
    "    num = sum((position!=0) & (change_pos!=0)) ## number of trades\n",
    "    hld_period = sum(position!=0)   ## holding period\n",
    "    daily_pnl = raw_pnl[end_day].reset_index(drop=True)\n",
    "    pnl = np.append(daily_pnl[0], np.diff(daily_pnl))\n",
    "    ## finally we combine the statistics into a data frame\n",
    "    #result = pd.DataFrame({\"final.pnl\": final_pnl, \"turnover\": turnover, \"num\": num, \"hld.period\": hld_period}, index=[0])\n",
    "    #result = {\"date\": date, \"final.pnl\": final_pnl, \"turnover\": turnover, \"num\": num, \"hld.period\": hld_period}\n",
    "    result = OrderedDict([(\"date\", date[end_day]), (\"pnl\", pnl),\n",
    "                          (\"final.pnl\", final_pnl), (\"turnover\", turnover), (\"num\", num), (\"hld.period\", hld_period)])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we use the parallel libary here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import functools\n",
    "with dask.config.set(scheduler='processes', num_workers=CORE_NUM):\n",
    "    f_par = functools.partial(get_daily_pnl, product=product_list[0], period=4096, threshold=0.001, noise=0)\n",
    "result_4 = compute([delayed(f_par)(date) for date in all_dates])[0]\n",
    "    \n",
    "## Wall time: 41.1 s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the col names and row names are opposite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(get_performance(result_4,1), index=[0])\n",
    "\n",
    "#  \tsharpe \tdrawdown \tmar \twin.ratio \tnum \tavg.pnl \thld.period\n",
    "#  \t3.320091 \t0.028157 \t35.514767 \t0.585656 \t4.579918 \t0.563022 \t724.568143"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we can see it's much better than before.\n",
    "- Keep in mind that it uses future data here so the result is not realistic\n",
    "- we just want to show that this scheme is much better than the previous one\n",
    "- and we would use this backtest method in the future course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import functools\n",
    "with dask.config.set(scheduler='processes', num_workers=CORE_NUM):\n",
    "    f_par = functools.partial(get_daily_pnl, product=product_list[0], period=4096, threshold=0.002, noise=0)\n",
    "result_5 = compute([delayed(f_par)(date) for date in all_dates])[0]\n",
    "\n",
    "# Wall time: 40.9 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(get_performance(result_5,1), index=[0])\n",
    "\n",
    "# sharpe \tdrawdown \tmar \twin.ratio \tnum \tavg.pnl \thld.period\n",
    "# \t6.090789 \t0.006768 \t147.756083 \t0.697541 \t2.593852 \t1.889284 \t1279.356928"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Previous method use fix size to trade, \n",
    "- now we suppose fix notional value of each trade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## daily pnl of fixed capital\n",
    "from collections import OrderedDict\n",
    "def get_daily_pnl(date, product=\"rb\", period=2000, threshold=0.001, buy_tranct=1.5e-4,sell_tranct=11.5e-4, noise=0, notional=False):\n",
    "    with gzip.open(dire+\"/\"+date, 'rb', compresslevel=1) as file_object:\n",
    "        raw_data = file_object.read()\n",
    "    data = cPickle.loads(raw_data)\n",
    "    n_bar = len(data)\n",
    "    unit = np.std(data[\"ret\"])\n",
    "    np.random.seed(10)\n",
    "    noise_ret = np.random.normal(scale=unit*noise, size=n_bar)\n",
    "    ##  we repeat the above code to get daily result\n",
    "    ret_2000 = (data[\"ret\"].rolling(period).sum()).dropna().reset_index(drop=True)\n",
    "    ret_2000 = ret_2000.append(pd.Series([0]*(len(data)-len(ret_2000)))).reset_index(drop=True) + noise_ret\n",
    "    \n",
    "    signal = pd.Series([0] * n_bar)\n",
    "    signal[ret_2000>threshold] = 1\n",
    "    signal[ret_2000< -threshold] = -1\n",
    "    \n",
    "    position_pos = pd.Series([np.nan] * n_bar)\n",
    "    position_pos[0] = 0\n",
    "    position_pos[(signal==1) & (data[\"next.ask\"]>0) & (data[\"next.bid\"]>0)] = 1\n",
    "    position_pos[(ret_2000< -threshold) & (data[\"next.bid\"]>0)] = 0\n",
    "    position_pos.ffill(inplace=True)   \n",
    "    pre_pos = position_pos.shift(1)\n",
    "    position_pos[(position_pos==1) & (pre_pos==1)] = np.nan ## holding positio rather than trade, change to nan\n",
    "    position_pos[(position_pos==1)] = 1/data[\"next.ask\"][(position_pos==1)] ## use 1/price as trading volume\n",
    "    position_pos.ffill(inplace=True) \n",
    "    \n",
    "    position_neg = pd.Series([np.nan] * n_bar)\n",
    "    position_neg[0] = 0\n",
    "    position_neg[(signal==-1) & (data[\"next.ask\"]>0) & (data[\"next.bid\"]>0)] = -1\n",
    "    position_neg[(ret_2000> threshold) & (data[\"next.ask\"]>0)] = 0\n",
    "    position_neg.ffill(inplace=True)\n",
    "    pre_neg = position_neg.shift(1)\n",
    "    position_neg[(position_neg==-1) & (pre_neg==-1)] = np.nan  ## holding positio rather than trade, change to nan\n",
    "    position_neg[(position_neg==-1)] = -1/data[\"next.bid\"][(position_neg==-1)] ## use 1/price as trading volume\n",
    "    position_neg.ffill(inplace=True) ## replace nan by trading volume\n",
    "    \n",
    "    position = position_pos + position_neg\n",
    "    position[0]=0\n",
    "    position[n_bar-1] = 0\n",
    "    position[n_bar-2] = 0\n",
    "    \n",
    "    change_pos = position - position.shift(1)\n",
    "    change_pos[0] = 0\n",
    "    change_base = pd.Series([0] * n_bar)\n",
    "    \n",
    "    change_buy = change_pos>0\n",
    "    change_sell = change_pos<0\n",
    "    \n",
    "    change_base[change_buy] = data[\"next.ask\"][change_buy]*(1+buy_tranct)\n",
    "    change_base[change_sell] = data[\"next.bid\"][change_sell]*(1-sell_tranct)\n",
    "    \n",
    "    raw_pnl = -(change_base*change_pos).cumsum()+position*data[\"price\"]\n",
    "    date = np.array([x[0:10] for x in data[\"date.time\"]])\n",
    "    next_date = np.append(date[1:],'1')\n",
    "    end_day = date!=next_date\n",
    "    final_pnl = -sum(change_base*change_pos) ## total pnl, there is a negative sign, because selling get money and buying pay money\n",
    "    turnover = sum(change_base*abs(change_pos))\n",
    "    num = sum((position!=0) & (change_pos!=0)) ## number of trades\n",
    "    hld_period = sum(position!=0)   ## holding period\n",
    "    daily_pnl = raw_pnl[end_day].reset_index(drop=True)\n",
    "    pnl = np.append(daily_pnl[0], np.diff(daily_pnl))\n",
    "    ## finally we combine the statistics into a data frame\n",
    "    #result = pd.DataFrame({\"final.pnl\": final_pnl, \"turnover\": turnover, \"num\": num, \"hld.period\": hld_period}, index=[0])\n",
    "    #result = {\"date\": date, \"final.pnl\": final_pnl, \"turnover\": turnover, \"num\": num, \"hld.period\": hld_period}\n",
    "    result = OrderedDict([(\"date\", date[end_day]), (\"pnl\", pnl),\n",
    "                          (\"final.pnl\", final_pnl), (\"turnover\", turnover), (\"num\", num), (\"hld.period\", hld_period)])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " %%time\n",
    "import functools\n",
    "with dask.config.set(scheduler='processes', num_workers=CORE_NUM):\n",
    "    f_par = functools.partial(get_daily_pnl, product=product_list[0], period=4096, threshold=0.001, notional=True)\n",
    "result = compute([delayed(f_par)(date) for date in all_dates])[0]\n",
    "    \n",
    "# Wall time: 41.8 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(get_performance(result,1), index=[0])\n",
    "\n",
    "#  \tsharpe \tdrawdown \tmar \twin.ratio \tnum \tavg.pnl \thld.period\n",
    "# \t3.096664 \t0.047369 \t21.110743 \t0.585246 \t4.579918 \t0.000836 \t724.568143"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After adding some noise, the result is worse than before but still pretty good\n",
    "- the later courses we would use this backtest method to test our signals and straregies without any future data\n",
    "- the reuslt would be much worse than the previous ones but they are realistic\n",
    "- Homework\n",
    "- You can try this method on other products and comapre their results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "117px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "764px",
    "left": "0px",
    "right": "1669px",
    "top": "110px",
    "width": "251px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
