{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "seventh-metabolism",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev2 toc-item\"><a href=\"#Reference\" data-toc-modified-id=\"Reference-01\"><span class=\"toc-item-num\">0.1&nbsp;&nbsp;</span>Reference</a></div><div class=\"lev2 toc-item\"><a href=\"#XGBoost-for-Classification\" data-toc-modified-id=\"XGBoost-for-Classification-02\"><span class=\"toc-item-num\">0.2&nbsp;&nbsp;</span>XGBoost for Classification</a></div><div class=\"lev3 toc-item\"><a href=\"#Get-input-data\" data-toc-modified-id=\"Get-input-data-021\"><span class=\"toc-item-num\">0.2.1&nbsp;&nbsp;</span>Get input data</a></div><div class=\"lev3 toc-item\"><a href=\"#Add-g_i-and-h_i-to-data\" data-toc-modified-id=\"Add-g_i-and-h_i-to-data-022\"><span class=\"toc-item-num\">0.2.2&nbsp;&nbsp;</span>Add $g_i$ and $h_i$ to data</a></div><div class=\"lev3 toc-item\"><a href=\"#Similarity-Computation\" data-toc-modified-id=\"Similarity-Computation-023\"><span class=\"toc-item-num\">0.2.3&nbsp;&nbsp;</span>Similarity Computation</a></div><div class=\"lev3 toc-item\"><a href=\"#Find-Optimal-Classification-Criteria-(cutoff-value-+-feature)\" data-toc-modified-id=\"Find-Optimal-Classification-Criteria-(cutoff-value-+-feature)-024\"><span class=\"toc-item-num\">0.2.4&nbsp;&nbsp;</span>Find Optimal Classification Criteria (cutoff value + feature)</a></div><div class=\"lev4 toc-item\"><a href=\"#Some-Sanity-Checks\" data-toc-modified-id=\"Some-Sanity-Checks-0241\"><span class=\"toc-item-num\">0.2.4.1&nbsp;&nbsp;</span>Some Sanity Checks</a></div><div class=\"lev2 toc-item\"><a href=\"#Return-a-tree-by-gradient-boost,-and-data\" data-toc-modified-id=\"Return-a-tree-by-gradient-boost,-and-data-03\"><span class=\"toc-item-num\">0.3&nbsp;&nbsp;</span>Return a tree by gradient boost, and data</a></div><div class=\"lev3 toc-item\"><a href=\"#Build-First-Tree\" data-toc-modified-id=\"Build-First-Tree-031\"><span class=\"toc-item-num\">0.3.1&nbsp;&nbsp;</span>Build First Tree</a></div><div class=\"lev3 toc-item\"><a href=\"#Build-Second-Tree\" data-toc-modified-id=\"Build-Second-Tree-032\"><span class=\"toc-item-num\">0.3.2&nbsp;&nbsp;</span>Build Second Tree</a></div><div class=\"lev3 toc-item\"><a href=\"#Put-Together\" data-toc-modified-id=\"Put-Together-033\"><span class=\"toc-item-num\">0.3.3&nbsp;&nbsp;</span>Put Together</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepared-sperm",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endangered-village",
   "metadata": {},
   "source": [
    "In this Jupyter, we follow the article in the below blog and replicate their results:\n",
    "\n",
    "- use their data as input to run regression\n",
    "- build tree with recursion\n",
    "- print out tree with BFS (level by level)\n",
    "- as an illustration, the hyper-parameters are quite simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upper-tonight",
   "metadata": {},
   "source": [
    "https://img-blog.csdn.net/2018031022534893?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMjIyMzg1MzM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-pillow",
   "metadata": {},
   "source": [
    "Tianqi Chen's XGBoost Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loose-inquiry",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/1603.02754.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "official-somerset",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "printable-trade",
   "metadata": {},
   "source": [
    "## XGBoost for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "healthy-rates",
   "metadata": {},
   "source": [
    "https://blog.csdn.net/qq_22238533/article/details/79477547"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chronic-developer",
   "metadata": {},
   "source": [
    "Let's follow TianQi's convention\n",
    "\n",
    "$L^{(t)}(q) = -\\frac{1}{2} \\sum_{j=1}^T \\frac{(\\sum_{i \\in I_j} g_i)^2}{\\sum_{i \\in I_j} h_i + \\lambda}$\n",
    "\n",
    "Or\n",
    "\n",
    "$L^{(t)}(q) = -\\frac{1}{2} \\sum_{j=1}^T (\\sum_{i \\in I_j} g_i)^2/ ( \\sum_{i \\in I_j} h_i + \\lambda)$\n",
    "\n",
    "Here, $T$ is the total number of leaves, or regions in the tree. $i$ is the index of each data. For regression\n",
    "\n",
    "$g_i = (y_i - \\hat{y}_i)$\n",
    "\n",
    "$h_i = \\text{\"previous probabilities\"*(1-\"previous probabilities\")}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indonesian-point",
   "metadata": {},
   "source": [
    "### Get input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-variable",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_g(df):\n",
    "    '''\n",
    "    Compute 1st derivative with previous probability and target value\n",
    "    '''\n",
    "    g = - (df['y'] - df['prev_p'])    \n",
    "    return g\n",
    "def compute_h(df):\n",
    "    '''\n",
    "    Compute 2nd derivative with previous prob\n",
    "    '''\n",
    "    h = df['prev_p'] * (1 - df['prev_p'])\n",
    "    return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reported-hampton",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../XGBoost_Input.csv\")\n",
    "# Initialize the prediction with prob=0.5\n",
    "base_score = 0.5\n",
    "df['prev_p'] = base_score\n",
    "\n",
    "# Hyper-parameters\n",
    "LAMBDA = 1\n",
    "MAX_DEPTH = 3\n",
    "ETA = 0.1 # the learning rate\n",
    "GAMMA = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "packed-handling",
   "metadata": {},
   "source": [
    "### Add $g_i$ and $h_i$ to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decent-slovak",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['g'] = compute_g(df)\n",
    "df['h'] = compute_h(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-tooth",
   "metadata": {},
   "source": [
    "### Similarity Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "received-gathering",
   "metadata": {},
   "source": [
    "Note, for each observation we have $L_i$ and $L_{all} = \\sum_i^N L_{i}$\n",
    "\n",
    "$ L(y_i, p_i) = - y_i \\ln p_i - (1-y_i) \\ln (1-p_i)$\n",
    "\n",
    "$ L(y_i, \\hat{y}_i)  = y_i \\ln (1+e^{-\\hat{y}_i}) + (1-y_i) \\ln (1+e^{\\hat{y}_i})$\n",
    "\n",
    "Then it's 1st and 2nd order derivative in terms of $\\hat{y}_i$:\n",
    "\n",
    "$L^\\prime = g_i = \\partial_{\\hat{y}(t-1)} L(y_i, \\hat{y}^{(t-1)}) =  - (y_i - p_i)$\n",
    "\n",
    "$L^{\\prime \\prime} = h_i = \\partial^2_{\\hat{y}(t-1)} L(y_i, \\hat{y}^{(t-1)}) = p_i (1-p_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wound-recording",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaf_similarity(df, lambda_0):\n",
    "    '''\n",
    "    sum(gi)/(sum(hi) + lambda_0)\n",
    "    '''\n",
    "    #print(df)\n",
    "    numerator = pow(df['g'].sum(), 2)\n",
    "    denom = (df['h'].sum() + lambda_0)\n",
    "    #denom = (pow(df['h'].sum(), 2) + lambda_0)\n",
    "    return numerator/float(denom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parental-version",
   "metadata": {},
   "source": [
    "### Find Optimal Classification Criteria (cutoff value + feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "otherwise-support",
   "metadata": {},
   "source": [
    "**The Gamma check, and cover check has not been added yet!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integral-kenya",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_classifer(df, col='x', lambda_0=0, verbose=False):\n",
    "    '''\n",
    "    This function, check all the values of each feature (x1, x2), and check the similarity score\n",
    "    with cutoff (left leaves < cutoff; right leveas >= cutoff)\n",
    "    Specifically, if x1: [1, 2, 3, 4, 5]\n",
    "    x2: [10, 11, 12, 13, 14]\n",
    "    The cutoff is any one of the below that gives the greatest gain in similarity:\n",
    "\n",
    "    cutoff 1: x1(< 1, >= 1)\n",
    "    cutoff 2: x1(< 2, >= 2)\n",
    "    cutoff 3: x1(< 3, >= 3)\n",
    "    cutoff 4: x1(< 4, >= 4)\n",
    "    cutoff 5: x1(< 5, >= 5)\n",
    "    cutoff 6: x2(< 10, >= 10)\n",
    "    cutoff 7: x2(< 11, >= 11)\n",
    "    cutoff 8: x2(< 12, >= 12)\n",
    "    cutoff 9: x2(< 13, >= 13)\n",
    "    cutoff 10: x2(< 14, >= 14)\n",
    "\n",
    "    return: a dictionary contains information, best cutoff, similarity of left & right leaf, \n",
    "    the data in left & right leaf\n",
    "    '''\n",
    "    df = df.sort_values([col], ascending=[1])\n",
    "    df = df.reset_index(drop=True)\n",
    "    clssifications = []\n",
    "    max_classifier = {}\n",
    "    no_split = leaf_similarity(df, lambda_0)\n",
    "    ans_gain = float('-inf')\n",
    "    cutoffs = sorted(df[col].unique()) # add this line to replicate the result\n",
    "    # sometimes, there are two classifers, however, we sort them and select the later\n",
    "    for cutoff in cutoffs:\n",
    "        df_L = df.loc[df[col] < cutoff]\n",
    "        df_R = df.loc[np.invert(df[col] < cutoff)]\n",
    "        score_L = leaf_similarity(df_L, lambda_0)\n",
    "        score_R = leaf_similarity(df_R, lambda_0)\n",
    "        #cutoff = (df_L.iloc[-1][col] + df_R.iloc[0][col])/2.0\n",
    "        if verbose: print(cutoff)\n",
    "        gain = score_L + score_R - no_split\n",
    "        if verbose: print(\"original = {}, socre L={}, score R={}, gain={}\".format(no_split, score_L, score_R, gain))\n",
    "        if gain >= ans_gain:\n",
    "            ans_gain = gain\n",
    "            max_classifier = {'cutoff':cutoff, 'left':score_L, 'right':score_R, 'gain':gain, 'left_data':df_L, 'right_data':df_R}\n",
    "        #break\n",
    "    return max_classifier\n",
    "\n",
    "def find_feature_classifier(df, feature_cols, lambda_0, verbose=False):\n",
    "    best_cutoff_of_features = {\"gain\":float('-inf')}\n",
    "    for col in feature_cols:\n",
    "        temp = find_best_classifer(df, col=col, lambda_0=lambda_0, verbose=verbose)\n",
    "        if temp['gain'] >= best_cutoff_of_features['gain']:\n",
    "            best_cutoff_of_features = temp\n",
    "            best_cutoff_of_features['feature'] = col\n",
    "            \n",
    "    return best_cutoff_of_features\n",
    "\n",
    "def split_by_cutoff(df, cutoff_info):\n",
    "    cutoff = cutoff_info['cutoff']\n",
    "    feature = cutoff_info['feature']\n",
    "    mask = df[feature] < cutoff\n",
    "    df_L, df_R = df.loc[mask], df.loc[~mask]\n",
    "    return df_L, df_R\n",
    "\n",
    "def compute_weight(df, lambda_0=1, eta=0.1):\n",
    "    '''\n",
    "    Compute weight of a region, note, eta is the learning rate and is applied to scale the weight\n",
    "    '''\n",
    "    return -eta*df['g'].sum()/(df['h'].sum()+lambda_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiovascular-mitchell",
   "metadata": {},
   "source": [
    "#### Some Sanity Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-pioneer",
   "metadata": {},
   "source": [
    "We will use a function to recursively build the tree, however, in this section, we check some cutoff points for the given inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recorded-necessity",
   "metadata": {},
   "outputs": [],
   "source": [
    "level_1_cutoff_info = find_feature_classifier(df, ['x1', 'x2'], lambda_0=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "answering-negotiation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_L = [None] * MAX_DEPTH\n",
    "df_R = [None] * MAX_DEPTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-somalia",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_L[1], df_R[1] = split_by_cutoff(df, cutoff_info=level_1_cutoff_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vulnerable-scholarship",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_L[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regular-rainbow",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_weight(df_R[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mechanical-supervisor",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_R[1] # Right leaf has only one data, stop here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accredited-circuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# depth = 1\n",
    "\n",
    "# # check the similarity of one specific column\n",
    "# # \n",
    "# check1 = find_best_classifer(df_L[1], col='x1', lambda_0=LAMBDA, verbose=False)\n",
    "# print(check1['gain'], check1['cutoff']) # when we look at x1 column, and use x1 < 3, then we get maximum gain which is 0.2539\n",
    "\n",
    "# # check the similarity of one specific column\n",
    "# # \n",
    "# check1 = find_best_classifer(df_L[1], col='x2', lambda_0=LAMBDA, verbose=False)\n",
    "# print(check1['gain'], check1['cutoff']) # when we look at x1 column, and use x1 < 3, then we get maximum gain which is 0.2539"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indirect-duncan",
   "metadata": {},
   "outputs": [],
   "source": [
    "level_2_cutoff_info = find_feature_classifier(df_L[1], ['x1', 'x2'], lambda_0=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forty-spring",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "level_2_cutoff_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspended-supervision",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The depth = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prerequisite-recipe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_L[2], df_R[2] = split_by_cutoff(df_L[1], cutoff_info=level_2_cutoff_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "actual-juice",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_feature_classifier(df_R[2], ['x1'], lambda_0=1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retained-integral",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "jewish-spectacular",
   "metadata": {},
   "source": [
    "## Return a tree by gradient boost, and data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mediterranean-designation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode:\n",
    "    def __init__(self, val=0, left=None, right=None):\n",
    "        self.val = val\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "def buildTree(df, height=0):\n",
    "    '''\n",
    "    Build a tree with df data with differen features;\n",
    "    if the tree depth is reached, then stop split;\n",
    "    for all the internal nodes, save cutoff + feature as the val;\n",
    "    for leaves, save weight as the val\n",
    "    '''\n",
    "    if df.shape[0]<=1 or height >= MAX_DEPTH:\n",
    "        weight = compute_weight(df)\n",
    "        root = TreeNode()\n",
    "        root.val = {'weight':weight, 'depth':height}\n",
    "        return root\n",
    "    \n",
    "    curr_node = TreeNode()\n",
    "    cutoff_info = find_feature_classifier(df, feature_cols=['x1', 'x2'], lambda_0=1)\n",
    "    curr_node.val = {\"cutoff\":cutoff_info['cutoff'], 'depth':height, 'feature':cutoff_info['feature'] }\n",
    "    df_L, df_R = split_by_cutoff(df, cutoff_info=cutoff_info)\n",
    "    curr_node.left = buildTree(df_L.copy(), height+1)\n",
    "    curr_node.right = buildTree(df_R.copy(), height+1)\n",
    "    \n",
    "    return curr_node\n",
    "    \n",
    "def traversal_bfs(root: TreeNode):\n",
    "    '''\n",
    "    BFS to print out the tree by level\n",
    "    '''\n",
    "    from collections import deque\n",
    "    result = []\n",
    "    if root is None:\n",
    "        return []\n",
    "    queue = deque()\n",
    "    queue.append(root)\n",
    "    depth = 0\n",
    "    while queue:\n",
    "        current_level = []\n",
    "        length = len(queue)\n",
    "        for _ in range(length):\n",
    "            node = queue.popleft()\n",
    "            current_level.append(node.val)\n",
    "            if node.left is not None: queue.append(node.left)\n",
    "            if node.right is not None: queue.append(node.right)\n",
    "        result.append(current_level)\n",
    "    return result\n",
    "\n",
    "def update_prev(df, root):\n",
    "    df2 = df.copy()\n",
    "    f_prev = np.log(df2['prev_p']/(1-df2['prev_p']))\n",
    "    f_curr = df2.apply(f_tree, root=root, axis=1)    \n",
    "    df2['prev_p'] = 1-1/(1+np.exp(f_prev + f_curr))\n",
    "    return df2\n",
    "\n",
    "def f_tree(x, root):\n",
    "    if 'weight' in root.val:\n",
    "        return root.val['weight']\n",
    "    feature = root.val['feature']\n",
    "    if x[feature] < root.val['cutoff']:\n",
    "        return f_tree(x, root.left)\n",
    "    else:\n",
    "        return f_tree(x, root.right)\n",
    "\n",
    "def update_g_h_and_rebuild(df):\n",
    "    '''\n",
    "    df['prev_p'] --> add two df['h'], df['g']\n",
    "    return df, root\n",
    "    '''\n",
    "    df['g'] = compute_g(df)\n",
    "    df['h'] = compute_h(df)\n",
    "    root = buildTree(df)\n",
    "    return df, root"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infrared-beverage",
   "metadata": {},
   "source": [
    "### Build First Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinct-brisbane",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = buildTree(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-ultimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "traversal_bfs(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-enzyme",
   "metadata": {},
   "source": [
    "Note:\n",
    "Our base value $p=0.5 \\rightarrow \\hat{y}_0 = 0$ \n",
    "\n",
    "In addition, $p$ is a sigmod mapping of our weights\n",
    "\n",
    "$p = \\frac{1}{1+\\exp (\\sum_{i=0}^M\\hat{y}_i)}$\n",
    "\n",
    "The weights that we assigned to the leaves are in fact $\\hat{y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "waiting-politics",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "needed-cycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "catholic-intermediate",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = update_prev(df, root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improved-fundamental",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.set_index('ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "african-healing",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.float_format', '{:0.6f}'.format):\n",
    "    print(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naked-mortality",
   "metadata": {},
   "source": [
    "### Build Second Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forbidden-strike",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2, root2 = update_g_h_and_rebuild(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-yukon",
   "metadata": {},
   "outputs": [],
   "source": [
    "traversal_bfs(root2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-migration",
   "metadata": {},
   "source": [
    "### Put Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isolated-importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../XGBoost_Input.csv\")\n",
    "df = df.set_index('ID')\n",
    "df['prev_p'] = base_score\n",
    "\n",
    "LAMBDA = 1\n",
    "MAX_DEPTH = 3\n",
    "ETA = 0.1 # the learning rate\n",
    "GAMMA = 0\n",
    "base_score = 0.5\n",
    "for M_trees in range(1, 10):\n",
    "#M_trees = 2\n",
    "    for i in range(0, M_trees):\n",
    "        df, root = update_g_h_and_rebuild(df)\n",
    "        df = update_prev(df, root)\n",
    "    df['hat(y)'] = (df['prev_p'] > 0.5).astype(int)\n",
    "    print(\"M(# of tree) = {}\".format(M_trees))\n",
    "    print(\"Precision:\",(df['y']==df['hat(y)']).sum()/df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scientific-guinea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "different-reminder",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "245.625px",
    "width": "251.989px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
