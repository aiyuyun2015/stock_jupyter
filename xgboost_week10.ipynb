{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev2 toc-item\"><a href=\"#Add-Global-Path\" data-toc-modified-id=\"Add-Global-Path-01\"><span class=\"toc-item-num\">0.1&nbsp;&nbsp;</span>Add Global Path</a></div><div class=\"lev2 toc-item\"><a href=\"#Load-Old-Functions\" data-toc-modified-id=\"Load-Old-Functions-02\"><span class=\"toc-item-num\">0.2&nbsp;&nbsp;</span>Load Old Functions</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- last two weeks we used all kinds of linear regression models\n",
    "- including forward stepwise regression, lasso, ridge, and elastic net\n",
    "- this week we move on to multiple addictive regression models\n",
    "- such as gradient boosting trees\n",
    "- other models like random forest, bagging are quite similar\n",
    "\n",
    "\n",
    "- among them boosting tress are the most famous and powerful\n",
    "- there are several packages in this filed\n",
    "- the oldest one is gbm\n",
    "- the package h2o includes a parallel version of gbm\n",
    "- and a newer one which has the best performance on kaggle competition is xgboost\n",
    "- and currently Microsoft developed an even better package called lightgbm\n",
    "- these week we can try on these models to see whether they can acutally boost our performance\n",
    "\n",
    "\n",
    "- first we can install lightgbm from the zip file I sent to you\n",
    "- for linux users maybe you can follow the instruction online, and it would be easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/Laurae2/lgbdl\n",
    "# https://github.com/r-dbi/RPostgres/issues/110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stock_helper import *\n",
    "from stock_stats import *\n",
    "from product_info import *\n",
    "from imp import reload  \n",
    "import stock_helper\n",
    "import stock_stats\n",
    "reload(stock_helper)\n",
    "reload(stock_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Global Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEAD_PATH = './'\n",
    "DATA_PATH = HEAD_PATH + \"data/stocks/\"\n",
    "SAVE_PATH = HEAD_PATH + \"ckpt/\"    # most of signal timeseries is here\n",
    "TEMP_PATH = SAVE_PATH + \"tmp pkl/\"\n",
    "OUTPUT_PATH = HEAD_PATH + 'output'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Old Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## parallel generate the distribution of a signal\n",
    "def par_get_all_signal(signal_name, file_list, product, period, SAVE_PATH=\"e:/intern\"):\n",
    "    \"\"\"\n",
    "    Our new functions that overwrite the stocker_helper\n",
    "    \"\"\"\n",
    "    n_files = len(file_list)\n",
    "    all_signal = np.array([])\n",
    "    for file in file_list:\n",
    "        S = load(SAVE_PATH+\"/tmp pkl/\"+product+\"/\"+signal_name+\"/\"+file) ## signal\n",
    "\n",
    "#         good = load(SAVE_PATH+\"/good pkl/\"+product+\"/\"+file) ## good singal\n",
    "#         signal = S[good]\n",
    "        signal = S\n",
    "        moving_average(signal,period)\n",
    "        chosen = (np.arange(len(signal))+1) % period==0\n",
    "        all_signal = np.concatenate((all_signal, signal[chosen]), axis=0)\n",
    "    save(all_signal, SAVE_PATH+\"/all signal/\"+product+\".\"+signal_name+\".pkl\")\n",
    "\n",
    "    \n",
    "# calc pnl given a signal\n",
    "def get_signal_pnl(file, product, signal_name, thre_mat, reverse=1,  buy_tranct=1.5e-4, sell_tranct=11.5e-4,\n",
    "                   max_spread=0.011,\n",
    "                   HEAD_PATH=HEAD_PATH, SAVE_PATH=SAVE_PATH, atr_filter=0):\n",
    "    ## load data\n",
    "    data = load(DATA_PATH+product+\"/\"+file)\n",
    "    S = load(SAVE_PATH+\"/tmp pkl/\"+product+\"/\"+signal_name+\"/\"+file)\n",
    "    pred = S*reverse\n",
    "    atr = load(SAVE_PATH+\"/tmp pkl/\"+product+\"/\"+\"atr.4096\"+\"/\"+file)\n",
    "    #n_bar = len(data)\n",
    "    \n",
    "    ## load signal\n",
    "    \n",
    "    ## we don't know the signal is positive correlated or negative correlated  \n",
    "    #n_thre = len(thre_mat)\n",
    "    date = np.array([x[0:10] for x in data[\"date.time\"]])\n",
    "    next_date = np.append(date[1:],'1')\n",
    "    end_day = date!=next_date\n",
    "    count = 0;\n",
    "    n_day = sum(end_day)\n",
    "    n_thre = np.shape(thre_mat)[0]\n",
    "    all_pnl = np.zeros((n_day, n_thre))\n",
    "    result = pd.DataFrame(data=OrderedDict([(\"open\", thre_mat[\"open\"].values), (\"close\", thre_mat[\"close\"].values),\n",
    "                               (\"num\", 0), (\"avg.ret\", 0), (\"ret\", 0)]), \n",
    "                          index=thre_mat.index)\n",
    "\n",
    "    cur_spread = data[\"ask1\"]-data[\"bid1\"]\n",
    "    for thre in thre_mat.iterrows():\n",
    "        count = count+1\n",
    "        buy = pred>thre[1][\"open\"]\n",
    "        sell = pred<-thre[1][\"open\"]\n",
    "        signal = pd.Series(data=0, index=data.index)\n",
    "        position = signal.copy()\n",
    "        signal[buy] = 1\n",
    "        signal[sell] = -1\n",
    "        signal[atr<atr_filter]=0\n",
    "        scratch = -thre[1][\"close\"]\n",
    "        position_pos = pd.Series(data=np.nan, index=data.index)\n",
    "        position_pos.iloc[0] = 0\n",
    "        position_pos[(signal==1) & (data[\"next.ask\"]>0) & (data[\"next.bid\"]>0) & (cur_spread<max_spread)] = 1\n",
    "        position_pos[(pred< -scratch) & (data[\"next.bid\"]>0) & (cur_spread<max_spread)] = 0\n",
    "        position_pos.ffill(inplace=True)\n",
    "        pre_pos = position_pos.shift(1)\n",
    "        notional_position_pos = pd.Series(data=0, index=data.index)\n",
    "        notional_position_pos[position_pos==1] = 1\n",
    "        notional_position_pos[(position_pos==1) & (pre_pos==1)] = np.nan\n",
    "        notional_position_pos[(notional_position_pos==1)] = 1/data[\"next.ask\"][(notional_position_pos==1)]\n",
    "        notional_position_pos.ffill(inplace=True)\n",
    "        position_neg = pd.Series(data=np.nan, index=data.index)\n",
    "        position_neg.iloc[0] = 0\n",
    "        position_neg[(signal==-1) & (data[\"next.ask\"]>0) & (data[\"next.bid\"]>0) & (cur_spread<max_spread)] = -1\n",
    "        position_neg[(pred> scratch) & (data[\"next.ask\"]>0) & (cur_spread<max_spread)] = 0\n",
    "        position_neg.ffill(inplace=True)\n",
    "        pre_neg = position_neg.shift(1)\n",
    "        notional_position_neg = pd.Series(data=0, index=data.index)\n",
    "        notional_position_neg[position_neg==-1] = -1\n",
    "        notional_position_neg[(position_neg==-1) & (pre_neg==-1)] = np.nan\n",
    "        notional_position_neg[(notional_position_neg==-1)] = -1/data[\"next.bid\"][(notional_position_neg==-1)]\n",
    "        notional_position_neg.ffill(inplace=True)\n",
    "        position = position_pos + position_neg\n",
    "        notional_position = notional_position_pos+notional_position_neg\n",
    "        #position[n_bar-1] = 0\n",
    "        position.iloc[0] = 0\n",
    "        position.iloc[-2:] = 0\n",
    "        notional_position.iloc[0] = 0\n",
    "        notional_position.iloc[-2:] = 0\n",
    "        #change_pos = position - position.shift(1)\n",
    "        #notional_change_pos = notional_position-notional_position.shift(1)\n",
    "        change_pos = notional_position-notional_position.shift(1)\n",
    "        change_pos.iloc[0] = 0\n",
    "        #notional_change_pos.iloc[0] = 0\n",
    "        change_base = pd.Series(data=0, index=data.index)\n",
    "        change_buy = change_pos>0\n",
    "        change_sell = change_pos<0\n",
    "        change_base[change_buy] = data[\"next.ask\"][change_buy]*(1+buy_tranct)*data[\"adjust\"]\n",
    "        change_base[change_sell] = data[\"next.bid\"][change_sell]*(1-sell_tranct)*data[\"adjust\"]\n",
    "        raw_pnl = -(change_base*change_pos).cumsum()+notional_position*data[\"wpr\"]\n",
    "        final_pnl = -sum(change_base*change_pos) ## total pnl, there is a negative sign, because selling get money and buying pay money\n",
    "        turnover = sum(change_base*abs(change_pos))\n",
    "        num = sum((position!=0) & (change_pos!=0)) ## number of trades\n",
    "        hld_period = sum(position!=0)   ## holding period\n",
    "        daily_pnl = raw_pnl[end_day].reset_index(drop=True)\n",
    "        pnl = np.append(daily_pnl[0], np.diff(daily_pnl))\n",
    "        all_pnl[:,thre[0]] = pnl\n",
    "        if (num==0):\n",
    "            result.loc[thre[0], (\"num\",\"avg.ret\",\"ret\")] = (0,0,0)\n",
    "        else:\n",
    "            result.loc[thre[0],(\"num\", \"avg.ret\", \"ret\", )] = (num, final_pnl/num, final_pnl)\n",
    "    return OrderedDict([(\"all.pnl\", all_pnl), (\"result\", result), (\"date\", date[end_day])])\n",
    "\n",
    "    \n",
    "from collections import OrderedDict\n",
    "def get_signal_stat(signal_name, thre_mat, product, all_dates, CORE_NUM, split_str=\"2018\", reverse=1, \n",
    "                     atr_filter=0, HEAD_PATH=\"d:/intern\", SAVE_PATH=\"d:/intern\"):\n",
    "    train_sample = all_dates<split_str ## training samples\n",
    "    test_sample = all_dates>split_str ## testing samples\n",
    "    with dask.config.set(scheduler='processes', num_workers=CORE_NUM):\n",
    "        f_par = functools.partial(get_signal_pnl, product=product, signal_name=signal_name, thre_mat=thre_mat,\n",
    "                                 reverse=1,\n",
    "                                HEAD_PATH=HEAD_PATH, SAVE_PATH=SAVE_PATH,atr_filter=atr_filter)\n",
    "        train_result = compute([delayed(f_par)(file) for file in all_dates[train_sample]])[0] ## get training result\n",
    "    train_stat = get_hft_summary(train_result, thre_mat) ## get training result statistics\n",
    "    with dask.config.set(scheduler='processes', num_workers=CORE_NUM):\n",
    "        f_par = functools.partial(get_signal_pnl, product=product, signal_name=signal_name, thre_mat=thre_mat,\n",
    "                                 reverse=1,\n",
    "                                HEAD_PATH=HEAD_PATH, SAVE_PATH=SAVE_PATH,atr_filter=atr_filter)\n",
    "        test_result = compute([delayed(f_par)(file) for file in all_dates[test_sample]])[0] ## get testing result\n",
    "    test_stat = get_hft_summary(test_result, thre_mat) ## get testing result statistics\n",
    "    return OrderedDict([(\"train.stat\", train_stat), (\"test.stat\", test_stat)])\n",
    "\n",
    "    \n",
    "def evaluate_signal(signal, all_dates, product, min_pnl, min_num, \n",
    "                    CORE_NUM, HEAD_PATH=\"d:/intern\", SIGNAL_PATH=\"d:/intern\", period=4096, split_str=\"2018\", \n",
    "                    atr_filter=0, save_path=\"signal result\",reverse=0):\n",
    "    signal_name = signal+\".\"+str(period) ## signal name, with period\n",
    "    all_signal = load(SIGNAL_PATH+\"/all signal/\"+product+\".\"+signal_name+\".pkl\") ## get the distribution of the signal\n",
    "    open_list = np.quantile(abs(all_signal), np.append(np.arange(0.991,0.999,0.001),np.arange(0.9991,0.9999,0.0001))) ## open threshold\n",
    "    thre_list = []\n",
    "    for cartesian in itertools.product(open_list, np.array([0.2, 0.4, 0.6, 0.8, 1.0])): ## close threshold\n",
    "        thre_list.append((cartesian[0], -cartesian[0] * cartesian[1]))\n",
    "    thre_list = np.array(thre_list)\n",
    "    thre_mat = pd.DataFrame(data=OrderedDict([(\"open\", thre_list[:, 0]), (\"close\", thre_list[:, 1])])) ## threshold matrix\n",
    "    \n",
    "    if reverse>=0: ## trending signal\n",
    "        print(\"reverse=1\")\n",
    "        trend_signal_stat = get_signal_stat(signal_name, thre_mat, product, all_dates, CORE_NUM, split_str=split_str, reverse=1, \n",
    "                                    atr_filter=atr_filter, HEAD_PATH=HEAD_PATH, SAVE_PATH=SIGNAL_PATH)\n",
    "    if reverse<=0: ## reversal signal\n",
    "        print(\"reverse=-1\")\n",
    "        reverse_signal_stat = get_signal_stat(signal_name, thre_mat, product, all_dates, CORE_NUM, split_str=split_str, reverse=-1,\n",
    "                                            atr_filter=atr_filter, HEAD_PATH=HEAD_PATH, SAVE_PATH=SIGNAL_PATH)\n",
    "    if reverse==0: ## both trending and reversal\n",
    "        stat_result = OrderedDict([(\"trend.signal.stat\", trend_signal_stat), (\"reverse.signal.stat\", reverse_signal_stat)])    \n",
    "        save(stat_result, HEAD_PATH+\"/\"+save_path+\"/\"+product+\".\"+signal_name+\".pkl\")\n",
    "    elif reverse==1: ## just trend\n",
    "        save(trend_signal_stat, HEAD_PATH+\"/\"+save_path+\"/\"+product+\".\"+signal_name+\".trend.pkl\")\n",
    "    elif reverse==-1: ## just reversal\n",
    "        save(reverse_signal_stat, HEAD_PATH+\"/\"+save_path+\"/\"+product+\".\"+signal_name+\".reverse.pkl\")\n",
    "        \n",
    "# summarize pnl over thresholds\n",
    "def get_hft_summary(result, thre_mat):\n",
    "    n_thre = np.shape(thre_mat)[0]\n",
    "    all_pnl = np.zeros((0,n_thre))\n",
    "    all_dates = np.array([])\n",
    "    for i in range(len(result)):\n",
    "        all_pnl =  np.concatenate((all_pnl,result[i][\"all.pnl\"]),axis=0)\n",
    "        all_dates = np.append(all_dates, result[i][\"date\"])\n",
    "    stat = result[0][\"result\"].iloc[:,2:]\n",
    "    for i in range(1,len(result)):\n",
    "        stat = stat+result[i][\"result\"].iloc[:,2:]\n",
    "    stat[\"avg.ret\"]=stat[\"ret\"]/stat[\"num\"]\n",
    "    \n",
    "    total_ret = all_pnl.sum(0)\n",
    "    total_sharpe = np.zeros(n_thre)\n",
    "    total_drawdown = np.zeros(n_thre)\n",
    "    total_max_drawdown = np.zeros(n_thre)\n",
    "    for i in range(n_thre):\n",
    "        total_sharpe[i] = sharpe(all_pnl[:,i])\n",
    "        total_drawdown[i] = drawdown(all_pnl[:,i])\n",
    "        total_max_drawdown[i] = max_drawdown(all_pnl[:,i])\n",
    "    final_result = pd.DataFrame(data=OrderedDict([(\"open\", thre_mat[\"open\"]), (\"close\", thre_mat[\"close\"]), (\"num\", stat[\"num\"]),\n",
    "                                                  (\"avg.ret\", stat[\"avg.ret\"]), (\"total.ret\",total_ret), (\"sharpe\", total_sharpe),\n",
    "                                                  (\"drawdown\", total_drawdown), (\"max.drawdown\", total_max_drawdown),\n",
    "                                                 (\"mar\", total_ret/total_max_drawdown)]), \n",
    "                                index=thre_mat.index)\n",
    "    return OrderedDict([(\"final.result\", final_result), (\"daily.pnl\", all_pnl), (\"date\", all_dates)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    " \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_platform():\n",
    "    import sys\n",
    "    platforms = {\n",
    "        'linux1' : 'Linux',\n",
    "        'linux2' : 'Linux',\n",
    "        'darwin' : 'OS X',\n",
    "        'win32' : 'Windows'\n",
    "    }\n",
    "    if sys.platform not in platforms:\n",
    "        return sys.platform\n",
    "    \n",
    "    return platforms[sys.platform]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_platform() != 'OS X':\n",
    "    CORE_NUM = int(os.environ['NUMBER_OF_PROCESSORS'])\n",
    "else:\n",
    "    import multiprocessing\n",
    "    CORE_NUM = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORE_NUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_list = [\"600519\", \"000858\", \"000568\", \"600809\", \"002304\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "period = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dates = os.listdir(DATA_PATH + product_list[0])\n",
    "all_dates.sort()\n",
    "all_dates = np.array(all_dates)\n",
    "n_days = len(all_dates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask import compute, delayed\n",
    "import sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dates_x = os.listdir(DATA_PATH + product_list[0])\n",
    "all_dates_y = os.listdir(DATA_PATH + product_list[1])\n",
    "all_dates = np.array(list(set(all_dates_x) & set(all_dates_y)))\n",
    "all_dates.sort()\n",
    "train_sample = all_dates<\"2017\"\n",
    "test_sample = all_dates>\"2017\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(HEAD_PATH+\"/all signal\", exist_ok=True)\n",
    "dire_signal_list = [\"nr\", \"dbook\", \"range.pos\", \"price.osci\", \"ma.dif.10\", \"kdj.k\", \"kdj.j\"]\n",
    "range_signal_list = [\"\", \"range\", \"std\", \"trend.index\"]\n",
    "all_signal_list = np.array([])\n",
    "for range_signal in range_signal_list:\n",
    "    for dire_signal in dire_signal_list:\n",
    "        if len(range_signal)==0:\n",
    "            signal_name = dire_signal\n",
    "        else:\n",
    "            signal_name = dire_signal+\".\"+range_signal\n",
    "        all_signal_list = np.append(all_signal_list,signal_name)\n",
    "signal_list = [signal+\".4096\" for signal in all_signal_list]\n",
    "np.append(signal_list, [\"ret.4096\", \"ret.4096.001\", \"ret.4096.002\"])\n",
    "n_signal = len(signal_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import lasso_path, LassoCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "y_signal = \"ret.\"+str(period)+\".002\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- let's begin with week 10\n",
    "- we can load xgboost and lightgbm at first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_mat = load(SAVE_PATH+\"/train test mat/\"+product_list[0]+\".train.mat.pkl\")\n",
    "signal_names = ori_mat.columns[0:-3]\n",
    "signal_names\n",
    "col_names = ori_mat.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_mat = load(SAVE_PATH+\"/train test mat/\"+product_list[0]+\".train.mat.pkl\")\n",
    "\n",
    "print(product_list[0], train_mat.shape)\n",
    "for product in product_list[1:]:\n",
    "    cur_mat = load(SAVE_PATH+\"/train test mat/\"+product+\".train.mat.pkl\")\n",
    "    cur_mat = pd.DataFrame(data=sklearn.preprocessing.scale(cur_mat, with_mean=False), columns=col_names)\n",
    "    print(product, cur_mat.shape)\n",
    "    train_mat = train_mat.append(cur_mat)\n",
    "\n",
    "# 600519 (1001, 32)\n",
    "# 000858 (1281, 32)\n",
    "# 000568 (1007, 32)\n",
    "# 600809 (756, 32)\n",
    "# 002304 (838, 32)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_std_mat = dict([])\n",
    "train_mat = load(SAVE_PATH+\"/train test mat/\"+product_list[0]+\".train.mat.pkl\")\n",
    "train_mat = train_mat[0:0]\n",
    "test_mat = train_mat\n",
    "for product in product_list:\n",
    "    cur_train_mat = load(SAVE_PATH+\"/train test mat/\"+product+\".train.mat.pkl\")\n",
    "    train_std_mat[product] = np.std(cur_train_mat)\n",
    "    train_mat = train_mat.append(cur_train_mat/train_std_mat[product])\n",
    "    cur_test_mat = load(SAVE_PATH+\"/train test mat/\"+product+\".test.mat.pkl\")\n",
    "    test_mat = test_mat.append(cur_test_mat/train_std_mat[product])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_mat.shape)\n",
    "print(test_mat.shape)\n",
    "\n",
    "# (4883, 32)\n",
    "# (4845, 32)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_mat.iloc[:, :n_signal].values\n",
    "y_train = train_mat.loc[:, y_signal].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat = \"gbm.wine.4096\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- create a directory for the first strate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for product in product_list:\n",
    "    #os.makedirs(SAVE_PATH+\"/tmp pkl/\"+product+\"/\"+strat, exist_ok=True)\n",
    "    os.makedirs(TEMP_PATH+product+\"/\"+strat, exist_ok=True)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb1 = GradientBoostingRegressor(random_state=0)\n",
    "param_grid = {\n",
    "        'n_estimators': np.arange(10,200,10),\n",
    "        'max_depth': [3,4,5,6,7,8,9,10],\n",
    "        'learning_rate':[0.01,0.1],\n",
    "        'min_samples_leaf': [10,20], \n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gbm1 = GridSearchCV(gb1, param_grid, n_jobs=CORE_NUM, cv=5);\n",
    "gbm1.fit(x_train, y_train);\n",
    "\n",
    "# Wall time: 17min 24s\n",
    "\n",
    "# GridSearchCV(cv=5, error_score='raise-deprecating',\n",
    "#              estimator=GradientBoostingRegressor(alpha=0.9,\n",
    "#                                                  criterion='friedman_mse',\n",
    "#                                                  init=None, learning_rate=0.1,\n",
    "#                                                  loss='ls', max_depth=3,\n",
    "#                                                  max_features=None,\n",
    "#                                                  max_leaf_nodes=None,\n",
    "#                                                  min_impurity_decrease=0.0,\n",
    "#                                                  min_impurity_split=None,\n",
    "#                                                  min_samples_leaf=1,\n",
    "#                                                  min_samples_split=2,\n",
    "#                                                  min_weight_fraction_leaf=0.0,\n",
    "#                                                  n_estimators=100,\n",
    "#                                                  n_iter...\n",
    "#                                                  subsample=1.0, tol=0.0001,\n",
    "#                                                  validation_fraction=0.1,\n",
    "#                                                  verbose=0, warm_start=False),\n",
    "#              iid='warn', n_jobs=16,\n",
    "#              param_grid={'learning_rate': [0.01, 0.1],\n",
    "#                          'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
    "#                          'min_samples_leaf': [10, 20],\n",
    "#                          'n_estimators': array([ 10,  20,  30,  40,  50,  60,  70,  80,  90, 100, 110, 120, 130,\n",
    "#        140, 150, 160, 170, 180, 190])},\n",
    "#              pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
    "#              scoring=None, verbose=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Gradient boosted tree regression...')\n",
    "print('Best Params:')\n",
    "print(gbm1.best_params_)\n",
    "print('Best CV Score:')\n",
    "print(gbm1.best_score_)\n",
    "save(gbm1, HEAD_PATH+\"/wine.gbm.4096.pkl\")\n",
    "\n",
    "# Gradient boosted tree regression...\n",
    "# Best Params:\n",
    "# {'learning_rate': 0.01, 'max_depth': 3, 'min_samples_leaf': 20, 'n_estimators': 120}\n",
    "# Best CV Score:\n",
    "# 0.0016858221075438632\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm1 = load(HEAD_PATH+\"/wine.gbm.4096.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for product in product_list:\n",
    "    parLapply(CORE_NUM, all_dates, get_daily_gbm, \n",
    "          product=product, signal_list=signal_list, \n",
    "          model=gbm1, strat=strat, HEAD_PATH=HEAD_PATH, SAVE_PATH=SAVE_PATH, train_std=np.array(train_std_mat[product][:n_signal]))\n",
    ";\n",
    "# Wall time: 6min 19s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_name = strat\n",
    "print(strat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for product in product_list:\n",
    "    par_get_all_signal(strat, all_dates, product, 4096, SAVE_PATH=SAVE_PATH)\n",
    "\n",
    "## Wall time:2.76 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# TODO: why all signal? use to be signal result atr\n",
    "for product in product_list:\n",
    "    evaluate_signal(\"gbm.wine\", all_dates, product, 0.001, 20, CORE_NUM, HEAD_PATH, SAVE_PATH,\n",
    "                        period=4096, split_str=\"2017\",atr_filter=0.02, save_path=\"ckpt/all signal\", reverse=1)\n",
    "\n",
    "\n",
    "# reverse=1\n",
    "# reverse=1\n",
    "# reverse=1\n",
    "# reverse=1\n",
    "# reverse=1\n",
    "# Wall time: 23min 22s\n",
    "# Wall time: 23min 22s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for product in product_list:\n",
    "    signal_stat = load(\"ckpt/all signal/\"+product+\".\"+\"gbm.wine.4096.trend\"+\".pkl\")\n",
    "    train_stat = signal_stat[\"train.stat\"]\n",
    "    good_strat = (train_stat[\"final.result\"][\"avg.ret\"]>0.001)\n",
    "    train_pnl = train_stat[\"daily.pnl\"][:, good_strat].sum(axis=1)/sum(good_strat)\n",
    "    test_stat = signal_stat[\"test.stat\"]\n",
    "    test_pnl = test_stat[\"daily.pnl\"][:, good_strat].sum(axis=1)/sum(good_strat)\n",
    "    print(product, \"train sharpe \", sharpe(train_pnl), \"test sharpe \", sharpe(test_pnl))\n",
    "\n",
    "# 600519 train sharpe  0.22919983414628217 test sharpe  -0.21748311895543404\n",
    "# 000858 train sharpe  0.18525756029068166 test sharpe  -1.2281289356720344\n",
    "# 000568 train sharpe  0.22594233445858758 test sharpe  -0.7105898416531242\n",
    "# 600809 train sharpe  0.3274442411746926 test sharpe  -1.1298196939247898\n",
    "# 002304 train sharpe  0.4769715509858891 test sharpe  -0.6685253299035706\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## construct the signal matrix\n",
    "## we want to put multiple signals into a matrix\n",
    "def get_sample_signal(good_night_files, sample, product, signal_list, period, daily_num):\n",
    "    n_samples = sum(daily_num[sample]) ## tottal number of samples\n",
    "    n_signal = len(signal_list) ## number of signals, the matrix would be n_samples*n_signal\n",
    "    all_signal =  np.ndarray(shape=(int(n_samples),n_signal))\n",
    "    cur = 0\n",
    "    for file in good_night_files[sample]:\n",
    "        data = load(DATA_PATH+product+\"/\"+file)\n",
    "        chosen = (np.arange(np.shape(data)[0])+1) % period==0\n",
    "        n_chosen = sum(chosen)\n",
    "        for i in range(n_signal):\n",
    "            signal_name = signal_list[i]\n",
    "            #S = load(SAVE_PATH+\"/tmp pkl/\"+product+\"/\"+signal_name+\"/\"+file)\n",
    "            S = load(TEMP_PATH + product + \"/\" + signal_name + \"/\" + file)\n",
    "            signal = S[(np.arange(len(S))+1) % period == 0]\n",
    "            signal[np.isnan(signal)] = 0 ## the ret.cor has some bad records\n",
    "            signal[np.isinf(signal)] = 0 ## the ret.cor has some bad records\n",
    "            all_signal[cur:(cur+n_chosen),i] = signal\n",
    "        cur = cur+n_chosen\n",
    "    all_signal = pd.DataFrame(all_signal, columns=signal_list)\n",
    "    return all_signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now we can go on with rolling gbm model\n",
    "- it's added this term of course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## rolling model for gradient boosting machine\n",
    "## returned value is the training sample standard deviation and the final model\n",
    "def get_multiple_gbm_roll_model(train_start, train_end, y_signal, strat, \n",
    "                          product_list, all_dates, daily_num, signal_list, period=4096, SAVE_PATH=SAVE_PATH):\n",
    "    scaler =  StandardScaler(copy=True, with_mean=False, with_std=True)\n",
    "    gb1 = GradientBoostingRegressor(random_state=0)\n",
    "    param_grid = {\n",
    "            'n_estimators': np.arange(20,300,20),\n",
    "            'max_depth': [3,4,5,6,7,8],\n",
    "            'learning_rate':[0.01],\n",
    "            'min_samples_leaf': [10], \n",
    "    }\n",
    "    model = GridSearchCV(gb1, param_grid, n_jobs=CORE_NUM, cv=5);\n",
    "    coef_list = dict([])\n",
    "    train_std_mat = dict([])\n",
    "    y_std = dict([])\n",
    "    n_signal = len(signal_list)\n",
    "    train_mat = np.zeros((0,n_signal))\n",
    "    y_train = np.array([])\n",
    "    train_start = 0\n",
    "    train_end = 2\n",
    "    sample  =(all_dates>=all_dates[train_start]) & (all_dates<=all_dates[train_end])\n",
    "    for product in product_list: ## combine data sets together and then fit the model\n",
    "        print(product)\n",
    "        train = get_sample_signal(all_dates, sample, product, np.append(signal_list, y_signal), period, daily_ticks[product])\n",
    "        x_train = train.iloc[:,:n_signal]\n",
    "        scaler.fit(x_train)\n",
    "        cur_y_train = train[y_signal]\n",
    "        x_train = scaler.transform(x_train) ## normalize data before combine\n",
    "        train_std_mat[product] = np.sqrt(scaler.var_)\n",
    "        train_mat = np.append(train_mat,x_train, axis=0)\n",
    "        y_std[product] = np.std(cur_y_train)\n",
    "        y_train = np.append(y_train, cur_y_train/y_std[product])\n",
    "    model.fit(train_mat, y_train);\n",
    "    os.makedirs(SAVE_PATH+\"/model\", exist_ok=True)\n",
    "    save(model, SAVE_PATH+\"/model/\"+strat+\".pkl\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_ticks = dict([])\n",
    "all_mat = dict([])\n",
    "for product in product_list:\n",
    "    daily_num = load(SAVE_PATH+\"/daily num/\"+product+\".pkl\")\n",
    "    daily_ticks[product] = daily_num\n",
    "    test_mat = load(SAVE_PATH+\"/train test mat/\"+product+\".test.mat.pkl\")\n",
    "    train_mat = load(SAVE_PATH+\"/train test mat/\"+product+\".train.mat.pkl\")\n",
    "    all_mat[product] = pd.concat((train_mat, test_mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "strat = \"gbm.wine.4096\"\n",
    "y_signal = \"ret.4096.002\"\n",
    "train_end = 6\n",
    "for train_start in range(4):\n",
    "    print(all_dates[train_start])\n",
    "    get_multiple_gbm_roll_model(train_start, train_end, y_signal, strat+\".\"+all_dates[train_start], \n",
    "                          product_list, all_dates, daily_num, signal_list, SAVE_PATH=SAVE_PATH)\n",
    "    train_start = train_start+1\n",
    "    train_end = train_end+1\n",
    "\n",
    "# 2011.pkl\n",
    "# 600519\n",
    "# 000858\n",
    "# 000568\n",
    "# 600809\n",
    "# 002304\n",
    "# 2012.pkl\n",
    "# 600519\n",
    "# 000858\n",
    "# 000568\n",
    "# 600809\n",
    "# 002304\n",
    "# 2013.pkl\n",
    "# 600519\n",
    "# 000858\n",
    "# 000568\n",
    "# 600809\n",
    "# 002304\n",
    "# 2014.pkl\n",
    "# 600519\n",
    "# 000858\n",
    "# 000568\n",
    "# 600809\n",
    "# 002304\n",
    "# Wall time: 11min 16s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now we can generate values of each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_daily_gbm(file_name, product, signal_list, model, strat, HEAD_PATH, SAVE_PATH, train_std, thre=float('Inf')):\n",
    "    signal_mat = load(SAVE_PATH+\"/signal mat pkl/\"+product+\"/\"+file_name).T/train_std\n",
    "    S = model.predict(signal_mat)\n",
    "    S[np.abs(S)>thre] = 0\n",
    "    #save(S, SAVE_PATH+\"/tmp pkl/\"+product+\"/\"+strat+\"/\"+file_name)\n",
    "    save(S, TEMP_PATH + product + \"/\" + strat + \"/\" + file_name)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "start_month = 0\n",
    "train_end = 6\n",
    "for train_start in range(4):\n",
    "    print(all_dates[train_start])\n",
    "    strat_name = strat+\".\"+all_dates[train_start]\n",
    "    gbm1 = load(SAVE_PATH+\"/model/\"+strat_name+\".pkl\")\n",
    "    #gbm1 = load(SAVE_PATH+\"/model/\"+strat_name)\n",
    "\n",
    "    for product in product_list:\n",
    "        #os.makedirs(SAVE_PATH+\"/tmp pkl/\"+product+\"/\"+strat_name, exist_ok=True)\n",
    "        os.makedirs(TEMP_PATH+product+\"/\"+strat_name, exist_ok=True)\n",
    "        ## each model has training set from train_start to train_end, and a test set until train_end+forward_len, so \n",
    "        ## we need to generate values of all_dates[train_start:(train_end+forward_len)+1], plus 1 to include train_end+forward_len\n",
    "        parLapply(CORE_NUM, all_dates, get_daily_gbm, product=product, signal_list=signal_list, \n",
    "                  model=gbm1, strat=strat_name, HEAD_PATH=HEAD_PATH, SAVE_PATH=SAVE_PATH, train_std=np.array(train_std_mat[product][:n_signal]))\n",
    "    train_end = train_end+1\n",
    "\n",
    "# 2011.pkl\n",
    "# 2012.pkl\n",
    "# 2013.pkl\n",
    "# 2014.pkl\n",
    "# Wall time: 4min 30s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "strat = \"gbm.wine.4096\"\n",
    "train_end = 6\n",
    "for train_start in range(4):\n",
    "    print(all_dates[train_start])\n",
    "    ## we only generte distribution on training set, not on test set\n",
    "    for product in product_list:\n",
    "        par_get_all_signal(strat+\".\"+all_dates[train_start], all_dates[train_start:(train_end+1)], product, 4096, SAVE_PATH=SAVE_PATH)\n",
    "    train_end = train_end+1\n",
    "    \n",
    "\n",
    "# 2011.pkl\n",
    "# 2012.pkl\n",
    "# 2013.pkl\n",
    "# 2014.pkl\n",
    "# Wall time: 6.19 s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get rolling statistics of the strategy performance\n",
    "def get_roll_result(product,strat,train_range, train_end,  atr_filter=0, save_path=\"signal result atr\", \n",
    "                          HEAD_PATH=SAVE_PATH, SIGNAL_PATH=SAVE_PATH):\n",
    "    for train_start in range(train_range):\n",
    "        strat_name = strat+\".\"+all_dates[train_start]\n",
    "        print(product, strat_name)\n",
    "        all_signal = load(SIGNAL_PATH+\"/all signal/\"+product+\".\"+strat_name+\".pkl\")\n",
    "        open_list = np.quantile(abs(all_signal), np.append(np.arange(0.991,0.999,0.001),np.arange(0.9991,0.9999,0.0001)))\n",
    "        thre_list = []\n",
    "        for cartesian in itertools.product(open_list, np.array([0.2, 0.4, 0.6, 0.8, 1.0])):\n",
    "            thre_list.append((cartesian[0], -cartesian[0] * cartesian[1]))\n",
    "        thre_list = np.array(thre_list)\n",
    "        thre_mat = pd.DataFrame(data=OrderedDict([(\"open\", thre_list[:, 0]), (\"close\", thre_list[:, 1])]))\n",
    "        train_result = parLapply(CORE_NUM, all_dates[train_start:(train_end+1)], get_signal_pnl,\n",
    "                                 product=product, signal_name=strat_name, thre_mat=thre_mat, reverse=1,\n",
    "                                 HEAD_PATH=HEAD_PATH, SAVE_PATH=SIGNAL_PATH,atr_filter=atr_filter)\n",
    "        train_stat = get_hft_summary(train_result, thre_mat)\n",
    "        test_result = parLapply(CORE_NUM, all_dates[(train_end+1):(train_end+2)], get_signal_pnl, \n",
    "                            product=product, signal_name=strat_name, thre_mat=thre_mat, reverse=1,\n",
    "                            HEAD_PATH=HEAD_PATH, SAVE_PATH=SIGNAL_PATH, atr_filter=atr_filter)\n",
    "        test_stat = get_hft_summary(test_result, thre_mat)\n",
    "        stat_result = OrderedDict([(\"train.stat\", train_stat), (\"test.stat\", test_stat)])    \n",
    "        save(stat_result, HEAD_PATH+\"/\"+save_path+\"/\"+product+\".\"+strat_name+\".pkl\")\n",
    "        train_end = train_end+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "strat = \"gbm.wine.4096\"\n",
    "for product in product_list:\n",
    "    get_roll_result(product, strat,4,6, atr_filter=0.02)\n",
    "\n",
    "# 600519 gbm.wine.4096.2011.pkl\n",
    "# 600519 gbm.wine.4096.2012.pkl\n",
    "# 600519 gbm.wine.4096.2013.pkl\n",
    "# 600519 gbm.wine.4096.2014.pkl\n",
    "# 000858 gbm.wine.4096.2011.pkl\n",
    "# 000858 gbm.wine.4096.2012.pkl\n",
    "# 000858 gbm.wine.4096.2013.pkl\n",
    "# 000858 gbm.wine.4096.2014.pkl\n",
    "# 000568 gbm.wine.4096.2011.pkl\n",
    "# 000568 gbm.wine.4096.2012.pkl\n",
    "# 000568 gbm.wine.4096.2013.pkl\n",
    "# 000568 gbm.wine.4096.2014.pkl\n",
    "# 600809 gbm.wine.4096.2011.pkl\n",
    "# 600809 gbm.wine.4096.2012.pkl\n",
    "# 600809 gbm.wine.4096.2013.pkl\n",
    "# 600809 gbm.wine.4096.2014.pkl\n",
    "# 002304 gbm.wine.4096.2011.pkl\n",
    "# 002304 gbm.wine.4096.2012.pkl\n",
    "# 002304 gbm.wine.4096.2013.pkl\n",
    "# 002304 gbm.wine.4096.2014.pkl\n",
    "# Wall time: 1h 30min 16s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: why the results different?\n",
    "n_product = len(product_list)\n",
    "save_path = \"signal result atr\"\n",
    "strat = \"gbm.wine.4096\"\n",
    "start_month = 0\n",
    "min_pnl = 0.001\n",
    "min_num = 10\n",
    "\n",
    "train_end = 6\n",
    "for i_product in range(n_product):\n",
    "    all_pnl = np.array([])    \n",
    "    for train_start in range(4):\n",
    "        strat_name = strat+\".\"+all_dates[train_start]\n",
    "        product = product_list[i_product]\n",
    "        stat_result = load(SAVE_PATH+\"/\"+save_path+\"/\"+product+\".\"+strat_name+\".pkl\")\n",
    "        train_stat = stat_result[\"train.stat\"]\n",
    "        test_stat = stat_result[\"test.stat\"]\n",
    "        good_strat = (train_stat[\"final.result\"][\"avg.ret\"]>min_pnl) & (train_stat[\"final.result\"][\"num\"]>min_num)\n",
    "        train_pnl = train_stat[\"daily.pnl\"][:, good_strat].sum(axis=1)/sum(good_strat) \n",
    "        train_pnl[np.isnan(train_pnl)]=0\n",
    "        test_pnl = test_stat[\"daily.pnl\"][:, good_strat].sum(axis=1)/sum(good_strat)\n",
    "        test_pnl[np.isnan(test_pnl)]=0\n",
    "        all_pnl = np.append(all_pnl, test_pnl)\n",
    "    plt.figure(i_product, figsize=(16, 10))\n",
    "    plt.title(product_list[i_product])\n",
    "    plt.ylabel(\"pnl\")\n",
    "    plt.plot(all_pnl.cumsum());\n",
    "    print(product, \"sharpe: \", sharpe(all_pnl))\n",
    "    \n",
    "# 600519 sharpe:  0.30726873760820356\n",
    "# 000858 sharpe:  0.8924871261897995\n",
    "# 000568 sharpe:  0.6929074966143157\n",
    "# 600809 sharpe:  1.1465550163246363\n",
    "# 002304 sharpe:  -0.765025038090387\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now let's start with xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we need to create specific train and test data sets for xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cv_params = {'max_depth': [3,4,5,6,7,8,9,10], \n",
    "             'n_estimators': np.arange(10,200,10)}\n",
    "ind_params = {'learning_rate': 0.1,\n",
    "              \"min_samples_leaf\":[10,20],\n",
    "              'seed':100, 'objective': 'reg:linear'}\n",
    "xgb_cv = GridSearchCV(xgb.XGBRegressor(**ind_params), \n",
    "                            cv_params, \n",
    "                            cv = 5, n_jobs = -1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xgb_cv.fit(x_train, y_train);\n",
    "\n",
    "# Wall time: 14min 33s\n",
    "\n",
    "# GridSearchCV(cv=5, error_score='raise-deprecating',\n",
    "#              estimator=XGBRegressor(base_score=0.5, booster='gbtree',\n",
    "#                                     colsample_bylevel=1, colsample_bytree=1,\n",
    "#                                     gamma=0, importance_type='gain',\n",
    "#                                     learning_rate=0.1, max_delta_step=0,\n",
    "#                                     max_depth=3, min_child_weight=1,\n",
    "#                                     min_samples_leaf=[10, 20], missing=None,\n",
    "#                                     n_estimators=100, n_jobs=1, nthread=None,\n",
    "#                                     objective='reg:linear', random_state=0,\n",
    "#                                     reg_alpha=0, reg_lambda=1,\n",
    "#                                     scale_pos_weight=1, seed=100, silent=True,\n",
    "#                                     subsample=1),\n",
    "#              iid='warn', n_jobs=-1,\n",
    "#              param_grid={'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
    "#                          'n_estimators': array([ 10,  20,  30,  40,  50,  60,  70,  80,  90, 100, 110, 120, 130,\n",
    "#        140, 150, 160, 170, 180, 190])},\n",
    "#              pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
    "#              scoring=None, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_parameters, score, _ = max(xgb_cv.grid_scorer, key=lambda x: x[1])\n",
    "xgb_cv.best_params_\n",
    "## {'max_depth': 3, 'n_estimators': 30}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save(xgb_cv, \"e:/intern/xgb_crypto.pkl\")\n",
    "save(xgb_cv, SAVE_PATH+\"xgb_crypto.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_cv = load( SAVE_PATH + \"xgb_crypto.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat = \"xgb.wine.4096\"\n",
    "signal_name = strat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for product in product_list:\n",
    "    #os.makedirs(SAVE_PATH+\"/tmp pkl/\"+product+\"/\"+strat, exist_ok=True)\n",
    "    os.makedirs(TEMP_PATH+product+\"/\"+strat, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we can plot them to check the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for product in product_list:\n",
    "    parLapply(CORE_NUM, all_dates, get_daily_gbm, \n",
    "          product=product, signal_list=signal_list, \n",
    "          model=xgb_cv, strat=strat, HEAD_PATH=HEAD_PATH, SAVE_PATH=SAVE_PATH, train_std=np.array(train_std_mat[product][:n_signal]))\n",
    ";\n",
    "# Wall time: 7min 5s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for product in product_list:\n",
    "    par_get_all_signal(strat, all_dates, product, 4096, SAVE_PATH=SAVE_PATH)\n",
    "    \n",
    "#  Wall time:  773 ms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for product in product_list:\n",
    "    evaluate_signal(\"xgb.wine\", all_dates, product, 0.001, 20, CORE_NUM, HEAD_PATH, SAVE_PATH,\n",
    "                        period=4096, split_str=\"2017\",atr_filter=0.02, save_path=SAVE_PATH+\"signal result atr\", reverse=1)\n",
    "\n",
    "# reverse=1\n",
    "# reverse=1\n",
    "# reverse=1\n",
    "# reverse=1\n",
    "# reverse=1\n",
    "# Wall time: 49min 29s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for product in product_list:\n",
    "    signal_stat = load(SAVE_PATH+\"/signal result atr/\"+product+\".\"+\"xgb.wine.4096.trend\"+\".pkl\")\n",
    "    train_stat = signal_stat[\"train.stat\"]\n",
    "    good_strat = (train_stat[\"final.result\"][\"avg.ret\"]>0.001)\n",
    "    train_pnl = train_stat[\"daily.pnl\"][:, good_strat].sum(axis=1)/sum(good_strat)\n",
    "    test_stat = signal_stat[\"test.stat\"]\n",
    "    test_pnl = test_stat[\"daily.pnl\"][:, good_strat].sum(axis=1)/sum(good_strat)\n",
    "    print(product, \"train sharpe \", sharpe(train_pnl), \"test sharpe \", sharpe(test_pnl))\n",
    "\n",
    "# 600519 train sharpe  0.1388950546824526 test sharpe  -0.36275758109400175\n",
    "# 000858 train sharpe  0.6539396017038269 test sharpe  0.19379771958752676\n",
    "# 000568 train sharpe  0.5938268485645808 test sharpe  0.008024859731152694\n",
    "# 600809 train sharpe  0.3510481091807721 test sharpe  0.024465093844693718\n",
    "# 002304 train sharpe  0.3935124085400787 test sharpe  -0.538542431125311\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- SUMMARY\n",
    "\n",
    "\n",
    "- today we compare two tree-based models: gradient boosting machine and xgboost\n",
    "- but the result is pretty bad\n",
    "- you can try to tune the parameter to see whether we could improve it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "48px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
